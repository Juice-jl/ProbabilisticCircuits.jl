<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning · ProbabilisticCircuits.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-136089579-2', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juice-jl.github.io/ProbabilisticCircuits.jl/stable/manual/learning/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ProbabilisticCircuits.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ProbabilisticCircuits.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox" checked/><label class="tocitem" for="menuitem-3"><span class="docs-label">Manual</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../demo/">Quick Demo</a></li><li><a class="tocitem" href="../queries/">Queries</a></li><li class="is-active"><a class="tocitem" href>Learning</a><ul class="internal"><li><a class="tocitem" href="#Learn-a-Circuit"><span>Learn a Circuit</span></a></li><li><a class="tocitem" href="#Learning-a-circuit-from-missing-data"><span>Learning a circuit from missing data</span></a></li><li><a class="tocitem" href="#Learn-a-mixture-of-circuits"><span>Learn a mixture of circuits</span></a></li><li><a class="tocitem" href="#Learn-a-circuit-from-logical-constraints-and-data"><span>Learn a circuit from logical constraints and data</span></a></li><li><a class="tocitem" href="#Learning-an-ensemble-of-circuits"><span>Learning an ensemble of circuits</span></a></li><li><a class="tocitem" href="#Misc-Options"><span>Misc Options</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/public/">Public APIs</a></li><li><a class="tocitem" href="../../api/types/">Type Trees</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Internals</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/internals/loadsave/">LoadSave</a></li><li><a class="tocitem" href="../../api/internals/logistic/">Logistic Circuits</a></li><li><a class="tocitem" href="../../api/internals/probabilistic_circuits/">Probabilistic Circuits</a></li><li><a class="tocitem" href="../../api/internals/utils/">Utils</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Juice-jl/ProbabilisticCircuits.jl/blob/master/docs/src/manual/learning.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="man-learning"><a class="docs-heading-anchor" href="#man-learning">Learning</a><a id="man-learning-1"></a><a class="docs-heading-anchor-permalink" href="#man-learning" title="Permalink"></a></h1><p>In this section we provide few learning scenarios for circuits. In general, learning tasks for PCs can be separted into two categories: parameter learning and structure learning.</p><h2 id="Learn-a-Circuit"><a class="docs-heading-anchor" href="#Learn-a-Circuit">Learn a Circuit</a><a id="Learn-a-Circuit-1"></a><a class="docs-heading-anchor-permalink" href="#Learn-a-Circuit" title="Permalink"></a></h2><p>You can use <a href="../../api/public/#ProbabilisticCircuits.learn_circuit"><code>learn_circuit</code></a> to learn a probabilistic circuit from the data (both parameter and structure learning).</p><pre><code class="language-julia">using LogicCircuits
using ProbabilisticCircuits
train_x, valid_x, test_x = twenty_datasets(&quot;nltcs&quot;)

pc = learn_circuit(train_x; maxiter=100);

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. &quot; *
&quot;Train log-likelihood is $(log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 786 nodes, 545 parameters. Train log-likelihood is -6.2004490845654505&quot;</pre><h2 id="Learning-a-circuit-from-missing-data"><a class="docs-heading-anchor" href="#Learning-a-circuit-from-missing-data">Learning a circuit from missing data</a><a id="Learning-a-circuit-from-missing-data-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-a-circuit-from-missing-data" title="Permalink"></a></h2><p>You can use <a href="../../api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_circuit_miss-Tuple{Any}"><code>learn_circuit_miss</code></a> to learn a probabilistic circuit from missing data, i.e. some feature could be missing for each data point.</p><pre><code class="language-julia">train_x_miss = make_missing_mcar(train_x; keep_prob=0.9)
pc = learn_circuit_miss(train_x_miss; maxiter=100);

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. &quot; *
&quot;Train marginal-log-likelihood is $(marginal_log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 340 nodes, 262 parameters. Train marginal-log-likelihood is -6.6359158&quot;</pre><h2 id="Learn-a-mixture-of-circuits"><a class="docs-heading-anchor" href="#Learn-a-mixture-of-circuits">Learn a mixture of circuits</a><a id="Learn-a-mixture-of-circuits-1"></a><a class="docs-heading-anchor-permalink" href="#Learn-a-mixture-of-circuits" title="Permalink"></a></h2><p>We also support learning mixture of circuits using the Strudel algorithm (<a href="../../api/public/#ProbabilisticCircuits.learn_strudel"><code>learn_strudel</code></a>).</p><pre><code class="language-julia">using LogicCircuits
using ProbabilisticCircuits
using Statistics

train_x, valid_x, test_x = twenty_datasets(&quot;nltcs&quot;)

spc, component_weights, lls = learn_strudel(train_x; num_mix = 10, init_maxiter = 20, em_maxiter = 100);

&quot;SPC: $(num_nodes(spc)) nodes, $(num_parameters(spc)) parameters. &quot; *
&quot;Train log-likelihood is $(mean(lls))&quot;</code></pre><pre class="documenter-example-output">&quot;SPC: 305 nodes, 1960 parameters. Train log-likelihood is -5.982393071695548&quot;</pre><h2 id="Learn-a-circuit-from-logical-constraints-and-data"><a class="docs-heading-anchor" href="#Learn-a-circuit-from-logical-constraints-and-data">Learn a circuit from logical constraints and data</a><a id="Learn-a-circuit-from-logical-constraints-and-data-1"></a><a class="docs-heading-anchor-permalink" href="#Learn-a-circuit-from-logical-constraints-and-data" title="Permalink"></a></h2><p>There are several ways to learn a probabilistic circuit consistent with logical constraints. Juice currently supports the following:</p><ol><li>Compilation from an SDD;</li><li>Compilation from a BDD;</li><li>Relaxation through <a href="../../api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_psdd-Tuple{LogicCircuits.Bdd,LogicCircuits.Vtree,Integer,DataFrames.DataFrame}"><code>sample_psdd</code></a>.</li></ol><h3 id="Compilation-from-an-SDD"><a class="docs-heading-anchor" href="#Compilation-from-an-SDD">Compilation from an SDD</a><a id="Compilation-from-an-SDD-1"></a><a class="docs-heading-anchor-permalink" href="#Compilation-from-an-SDD" title="Permalink"></a></h3><p>A circuit (more specifically a PSDD) can be easily constructed from an SDD by simply calling the <code>compile</code> function.</p><p>Let&#39;s assume we have the following CNF</p><div>\[\phi(a,b,c,d)=(a\vee\neg b)\wedge(c\vee\neg d)\wedge(a\vee\neg d)\]</div><p>as a <a href="https://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html"><code>.cnf</code> file</a>:</p><pre><code class="language-none">/tmp/example.cnf
---
c Encodes the following CNF: ϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)
c
p cnf 4 3
1 -2 0
3 -4 0
1 -4 0</code></pre><p>First we construct an SDD from the CNF. Here we sample a random vtree as an example (you might want to learn it from data instead with <a href="manual/@ref"><code>learn_vtree</code></a>).</p><pre><code class="language-">using LogicCircuits
using ProbabilisticCircuits
using DataFrames

n = 4 # number of variables
V = Vtree(n, :random)
sdd = compile(SddMgr(V), load_cnf(&quot;/tmp/example.cnf&quot;))
pc = compile(StructProbCircuit, sdd)</code></pre><p>Let&#39;s check its support.</p><pre><code class="language-"># Matrix with all possible worlds.
M = BitMatrix(undef, 2^n, n)
for i in 1:size(M, 1) M[i,:] .= [c == &#39;0&#39; ? false : true for c in reverse(bitstring(i-1)[end-n-1:end])] end
display(M)

# Evaluate SDD.
display(sdd.(eachrow(M)))</code></pre><p>And now the probabilities:</p><pre><code class="language-"># Evaluate the PSDD support.
EVI(pc, DataFrame(M))</code></pre><h3 id="Compilation-from-a-BDD"><a class="docs-heading-anchor" href="#Compilation-from-a-BDD">Compilation from a BDD</a><a id="Compilation-from-a-BDD-1"></a><a class="docs-heading-anchor-permalink" href="#Compilation-from-a-BDD" title="Permalink"></a></h3><p>Compiing from a BDD is straightforward. Let&#39;s first create a BDD from the same previous constraints using <code>LogicCircuits</code>.</p><pre><code class="language-julia">using LogicCircuits

ϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)</code></pre><pre class="documenter-example-output">@ (index=1, id=6)
|  - (index=2, id=5)
|  |  - (index=4, id=3)
|  |  |  - (value=true, id=2)
|  |  |  + (value=false, id=1)
|  |  + (value=false, id=1)
|  + (index=3, id=4)
|  |  - (index=4, id=3)
|  |  |  - (value=true, id=2)
|  |  |  + (value=false, id=1)
|  |  + (value=true, id=2)
</pre><p>Now we either compile the PSDD directly from the BDD and give it random weights:</p><pre><code class="language-">using ProbabilisticCircuits, DataFrames

# Get all possible instances with BDD.all_valuations.
M = all_valuations(collect(1:n))
M_D = DataFrame(M)

pc = generate_from_bdd(ϕ, 4)
EVI(pc, M_D)</code></pre><p>Or compile from a BDD and learn weights from data:</p><pre><code class="language-"># Retrieve only possible worlds.
W = M[findall(ϕ.(eachrow(M))),:]
# Assign random probabilities for each world in W.
R = rand(1:20, size(W, 1))
# Construct a dataset that maps the distribution of R (world W[i] repeats R[i] times).
D = DataFrame(vcat([repeat(W[i,:], 1, R[i])&#39; for i ∈ 1:size(W, 1)]...))

pc = learn_bdd(ϕ, D; pseudocount = 0.0)
EVI(pc, M_D)</code></pre><p>Since BDDs are just right-linear vtree PSDDs, this &quot;compilation&quot; is merely a conversion from BDD syntax to PC syntax, attributing some weight to edges.</p><h3 id="Sampling-a-circuit-from-a-relaxation-of-the-constraints"><a class="docs-heading-anchor" href="#Sampling-a-circuit-from-a-relaxation-of-the-constraints">Sampling a circuit from a relaxation of the constraints</a><a id="Sampling-a-circuit-from-a-relaxation-of-the-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-a-circuit-from-a-relaxation-of-the-constraints" title="Permalink"></a></h3><p>The two previous approaches are effective, but not always adequate. For instance, suppose our data consists of 6 variables: <span>$a$</span>, <span>$b$</span>, <span>$c$</span>, <span>$d$</span>, <span>$e$</span> and <span>$f$</span>, where only <span>$a$</span>, <span>$b$</span>, <span>$c$</span> and <span>$d$</span> are constrained (by <span>$\phi$</span>), and the rest are free. Had we compiled <span>$\phi$</span> from either an SDD or BDD, we&#39;d end up with trivial structures for free variables. For instance, calling <a href="../../api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_bdd-Tuple{LogicCircuits.Bdd,DataFrames.DataFrame}"><code>learn_bdd</code></a> (or <a href="../../api/internals/probabilistic_circuits/#ProbabilisticCircuits.generate_from_bdd-Tuple{LogicCircuits.Bdd,Integer}"><code>generate_from_bdd</code></a>) with more variables than the size of the BDD&#39;s scope would result in a fully factorized distribution over the free variables.</p><p>To address these issues, we might want to generate a circuit from both free and constrained variables with <a href="../../api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_psdd-Tuple{LogicCircuits.Bdd,LogicCircuits.Vtree,Integer,DataFrames.DataFrame}"><code>sample_psdd</code></a>. Unfortunately, to keep the circuit tractable, <code>sample_psdd</code> only provides a relaxation of the constraints.</p><p>Let&#39;s first encode our constraints as a BDD just like our previous example and make up some random data.</p><pre><code class="language-">using LogicCircuits, DataFrames

n = 6
ϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)
M = all_valuations(collect(1:n))
M_D = DataFrame(M)
W = M[findall(ϕ.(eachrow(M))),:]
R = rand(1:20, size(W, 1))
D = DataFrame(vcat([repeat(W[i,:], 1, R[i])&#39; for i ∈ 1:size(W, 1)]...))</code></pre><p>Now we can sample circuits from <span>$\phi$</span> and data <span>$D$</span>.</p><pre><code class="language-">using ProbabilisticCircuits
using LogicCircuits: Vtree

pc = sample_psdd(ϕ, Vtree(n, :random), 16, D)
EVI(pc, M_D)</code></pre><p>The third argument passed to <code>sample_psdd</code> indicates an upper bound on the number of children whose parents are sum nodes. The higher this bound, the more consistent with <span>$\phi$</span>.</p><p>In situations where background knowledge is not available, we may pass <span>$\top$</span> to <code>sample_psdd</code> to only learn from data.</p><pre><code class="language-">pc = sample_psdd(⊤, Vtree(n, :random), 16, D)
EVI(pc, M_D)</code></pre><h2 id="Learning-an-ensemble-of-circuits"><a class="docs-heading-anchor" href="#Learning-an-ensemble-of-circuits">Learning an ensemble of circuits</a><a id="Learning-an-ensemble-of-circuits-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-an-ensemble-of-circuits" title="Permalink"></a></h2><p><a href="../../api/public/#ProbabilisticCircuits.learn_strudel"><code>learn_strudel</code></a> let&#39;s us learn an ensemble of circuits that share the same structure. For learning ensembles whose components have different structures, we have to use <code>Ensemble</code>.</p><h3 id="Ensemble-of-sample_psdds"><a class="docs-heading-anchor" href="#Ensemble-of-sample_psdds">Ensemble of <code>sample_psdd</code>s</a><a id="Ensemble-of-sample_psdds-1"></a><a class="docs-heading-anchor-permalink" href="#Ensemble-of-sample_psdds" title="Permalink"></a></h3><p>We can learn an ensemble of random circuits through <code>ensemble_sample_psdd</code>.</p><pre><code class="language-">E = ensemble_sample_psdd(10, ϕ, 16, D; strategy = :em)
EVI(E, M_D)</code></pre><p>Here we used EM to learn the weights of the ensemble. Alternatives are likelihood weighting (<code>:likelihood</code>), uniform weights (<code>:uniform</code>) or stacking (<code>:stacking</code>).</p><h2 id="Misc-Options"><a class="docs-heading-anchor" href="#Misc-Options">Misc Options</a><a id="Misc-Options-1"></a><a class="docs-heading-anchor-permalink" href="#Misc-Options" title="Permalink"></a></h2><p>In this sections, we provide options to have more control in learning circuits. For example, what if we only want to do parameter learning.</p><h3 id="Parameter-Learning"><a class="docs-heading-anchor" href="#Parameter-Learning">Parameter Learning</a><a id="Parameter-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Learning" title="Permalink"></a></h3><p>Given a fixed structure for the PC, the goal of parameter learning is to estimate the parameters so that likelihood is maximized.</p><p>First, initliaze PC structure with a balanced vtree represneting a fully factorized distribution:</p><pre><code class="language-julia">v = Vtree(num_features(train_x), :balanced)
pc = fully_factorized_circuit(StructProbCircuit, v);

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.&quot; *
&quot;Train log-likelihood is $(log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 79 nodes, 48 parameters.Train log-likelihood is -11.090354888959103&quot;</pre><p>No parmater learning is done yet, now let&#39;s, do maximum likelihood estimatation (MLE) using <a href="../../api/public/#ProbabilisticCircuits.estimate_parameters"><code>estimate_parameters</code></a>:</p><pre><code class="language-julia">estimate_parameters(pc, train_x; pseudocount=1.0);

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.&quot; *
&quot;Train log-likelihood is $(log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 79 nodes, 48 parameters.Train log-likelihood is -9.270330518351884&quot;</pre><p>As we see the likelihood improved, however we are still using a fully factorized distribution. There is room for improvement. For example, we can choose initial structure based on Chow-Liu Trees.</p><pre><code class="language-julia">pc, vtree = learn_chow_liu_tree_circuit(train_x)

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.&quot; *
&quot;Train log-likelihood is $(log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 121 nodes, 74 parameters.Train log-likelihood is -6.7600561877106955&quot;</pre><h3 id="Structure-Learning"><a class="docs-heading-anchor" href="#Structure-Learning">Structure Learning</a><a id="Structure-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-Learning" title="Permalink"></a></h3><p>There are several different approaches in structure learning. Currently we support the following approach:</p><ol><li>Choose an initial structure and learn parameters</li><li>Perform Greedy search for a bigger and better structure by doing operations such as split and clone.</li><li>Repeat step 2 until satisfied or time limit reached</li></ol><p>We start with the Chow-Liu structure we learned in last section, and run few structure learning iterations (20):</p><pre><code class="language-julia">pc, vtree = learn_chow_liu_tree_circuit(train_x)
loss(circuit) = ProbabilisticCircuits.heuristic_loss(circuit, train_x)
pc = struct_learn(pc;
    primitives=[split_step],
    kwargs=Dict(split_step=&gt;(loss=loss,)),
    maxiter=20)
estimate_parameters(pc, train_x; pseudocount=1.0)

&quot;PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. &quot; *
&quot;Training set log-likelihood is $(log_likelihood_avg(pc, train_x))&quot;</code></pre><pre class="documenter-example-output">&quot;PC: 305 nodes, 196 parameters. Training set log-likelihood is -6.383045029183076&quot;</pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../queries/">« Queries</a><a class="docs-footer-nextpage" href="../../api/public/">Public APIs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 7 September 2021 07:02">Tuesday 7 September 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
