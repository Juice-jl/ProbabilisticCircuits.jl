{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460fe332",
   "metadata": {},
   "source": [
    "ProbabilisticCircuits.jl offers various parameter and structure learning algorithms for PCs. In this example, we will demonstrate how to generate a particular PC model termed *Hidden Chow-Liu Tree (HCLT)* and use it to train a density (generative) model with **SoTA** likelihood on MNIST in around **5 minutes**. In comparison, as shown in the table below, VAE and Flow models have worse likelihoods on MNIST and EMNIST. Additionally, they typically need several hours to train, which is over **10x slower than PCs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d5b0f",
   "metadata": {},
   "source": [
    "| Dataset | PC (ours) | IDF | Hierarchical VAE | PixelVAE |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| MNIST | **1.20** | 2.90 | 1.27 | 1.39 |\n",
    "| FashionMNIST | 3.34 | 3.47 | **3.28** | 3.66 |\n",
    "| EMNIST (Letter split) | **1.80** | 1.95 | 1.84 | 2.26 |\n",
    "| EMNIST (ByClass split) | **1.85** | 1.98 | 1.87 | 2.23 |\n",
    "\n",
    "\\* Note: all reported numbers are bits-per-dimension (bpd). The results are extracted from [1].\n",
    "\n",
    "[1] Anji Liu, Stephan Mandt and Guy Van den Broeck. Lossless Compression with Probabilistic Circuits, In International Conference on Learning Representations (ICLR), 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020cd1e",
   "metadata": {},
   "source": [
    "We start by importing ProbabilisticCircuits.jl and other required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb49152",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ProbabilisticCircuits\n",
    "using MLDatasets\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8eaef",
   "metadata": {},
   "source": [
    "We first load the MNIST dataset from MLDatasets.jl and move them to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e53a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset summary:\n",
      " - Number of training examples: 60000\n",
      " - Number of test examples: 10000\n",
      " - Number of features: 784\n"
     ]
    }
   ],
   "source": [
    "mnist_train_cpu = collect(transpose(reshape(MNIST.traintensor(UInt8), 28*28, :)))\n",
    "mnist_test_cpu = collect(transpose(reshape(MNIST.testtensor(UInt8), 28*28, :)))\n",
    "mnist_train_gpu = cu(mnist_train_cpu)\n",
    "mnist_test_gpu = cu(mnist_test_cpu)\n",
    "println(\"Dataset summary:\\n - Number of training examples: $(size(mnist_train_cpu, 1))\\n - Number of test examples: $(size(mnist_test_cpu, 1))\\n - Number of features: $(size(mnist_train_cpu, 2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a6a7f",
   "metadata": {},
   "source": [
    "We move on to generate the HCLT structure. `hclt` constructs a smooth and structured-decomposable PC whose structure depends on the input samples. Specifically, it computes the pairwise mutual information (MI) between the MNIST features (i.e., pixels), and use the pairwise MI matrix to determine the PC structure, such that highly correlated features are placed \"closer\" in the PC to facilitate learning. In the following, `bits` is the number of bits to truncate to speedup the pairwise MI computation, and `latents` specifies the size of the generated HCLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47d108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HCLT structure with 32 latents... \n",
      " 24.203915 seconds (82.55 M allocations: 6.716 GiB, 5.69% gc time, 50.04% compilation time)\n",
      "Number of free parameters: 6980767\n"
     ]
    }
   ],
   "source": [
    "bits = 4\n",
    "latents = 32\n",
    "println(\"Generating HCLT structure with $latents latents... \");\n",
    "trunc_train = cu(mnist_train_cpu .รท 2^bits)\n",
    "@time pc = hclt(trunc_train, latents; num_cats = 256, pseudocount = 0.1, input_type = Categorical)\n",
    "init_parameters(pc; perturbation = 0.4)\n",
    "println(\"Number of free parameters: $(num_parameters(pc))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81f93",
   "metadata": {},
   "source": [
    "To facilitate efficient parameter learning on GPUs, we first convert `pc` into an equivalent GPU-friendly low-level representation termed bits-circuit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02928e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving pc to GPU...   1.974239 seconds (17.74 M CPU allocations: 1.215 GiB, 7.59% gc time) (7 GPU allocations: 76.784 MiB, 0.00% memmgmt time)\n"
     ]
    }
   ],
   "source": [
    "print(\"Moving pc to GPU... \")\n",
    "CUDA.@time bpc = CuBitsProbCircuit(pc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec677c",
   "metadata": {},
   "source": [
    "We are now ready to train the parameters of the PC. This is done by calling the high-level API `mini_batch_em`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d51ee09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch EM epoch 1; train LL -913.08734\n",
      "Mini-batch EM epoch 2; train LL -847.4659\n",
      "Mini-batch EM epoch 3; train LL -841.1994\n",
      "Mini-batch EM epoch 4; train LL -836.8761\n",
      "Mini-batch EM epoch 5; train LL -833.25433\n",
      "Mini-batch EM epoch 6; train LL -830.4178\n",
      "Mini-batch EM epoch 7; train LL -827.6333\n",
      "Mini-batch EM epoch 8; train LL -825.24286\n",
      "Mini-batch EM epoch 9; train LL -822.84735\n",
      "Mini-batch EM epoch 10; train LL -820.6389\n",
      "Mini-batch EM epoch 11; train LL -818.42413\n",
      "Mini-batch EM epoch 12; train LL -816.32837\n",
      "Mini-batch EM epoch 13; train LL -814.23914\n",
      "Mini-batch EM epoch 14; train LL -812.36127\n",
      "Mini-batch EM epoch 15; train LL -810.39795\n",
      "Mini-batch EM epoch 16; train LL -808.6348\n",
      "Mini-batch EM epoch 17; train LL -806.7153\n",
      "Mini-batch EM epoch 18; train LL -804.8232\n",
      "Mini-batch EM epoch 19; train LL -803.06384\n",
      "Mini-batch EM epoch 20; train LL -801.2593\n",
      "Mini-batch EM epoch 21; train LL -799.64825\n",
      "Mini-batch EM epoch 22; train LL -797.8991\n",
      "Mini-batch EM epoch 23; train LL -796.13086\n",
      "Mini-batch EM epoch 24; train LL -794.2558\n",
      "Mini-batch EM epoch 25; train LL -792.8715\n",
      "Mini-batch EM epoch 26; train LL -791.07025\n",
      "Mini-batch EM epoch 27; train LL -789.5116\n",
      "Mini-batch EM epoch 28; train LL -787.9621\n",
      "Mini-batch EM epoch 29; train LL -786.31396\n",
      "Mini-batch EM epoch 30; train LL -784.80133\n",
      "Mini-batch EM epoch 31; train LL -783.13434\n",
      "Mini-batch EM epoch 32; train LL -781.7087\n",
      "Mini-batch EM epoch 33; train LL -780.1956\n",
      "Mini-batch EM epoch 34; train LL -778.58105\n",
      "Mini-batch EM epoch 35; train LL -777.0732\n",
      "Mini-batch EM epoch 36; train LL -775.52924\n",
      "Mini-batch EM epoch 37; train LL -774.0582\n",
      "Mini-batch EM epoch 38; train LL -772.5499\n",
      "Mini-batch EM epoch 39; train LL -771.0382\n",
      "Mini-batch EM epoch 40; train LL -769.5695\n",
      "Mini-batch EM epoch 41; train LL -768.07153\n",
      "Mini-batch EM epoch 42; train LL -766.61914\n",
      "Mini-batch EM epoch 43; train LL -765.2777\n",
      "Mini-batch EM epoch 44; train LL -763.65906\n",
      "Mini-batch EM epoch 45; train LL -762.2729\n",
      "Mini-batch EM epoch 46; train LL -760.8226\n",
      "Mini-batch EM epoch 47; train LL -759.24\n",
      "Mini-batch EM epoch 48; train LL -757.85034\n",
      "Mini-batch EM epoch 49; train LL -756.46704\n",
      "Mini-batch EM epoch 50; train LL -754.9396\n",
      "Mini-batch EM epoch 51; train LL -753.51025\n",
      "Mini-batch EM epoch 52; train LL -752.05383\n",
      "Mini-batch EM epoch 53; train LL -750.49445\n",
      "Mini-batch EM epoch 54; train LL -749.2557\n",
      "Mini-batch EM epoch 55; train LL -747.6557\n",
      "Mini-batch EM epoch 56; train LL -746.22473\n",
      "Mini-batch EM epoch 57; train LL -744.791\n",
      "Mini-batch EM epoch 58; train LL -743.35626\n",
      "Mini-batch EM epoch 59; train LL -741.9024\n",
      "Mini-batch EM epoch 60; train LL -740.4583\n",
      "Mini-batch EM epoch 61; train LL -739.07697\n",
      "Mini-batch EM epoch 62; train LL -737.49585\n",
      "Mini-batch EM epoch 63; train LL -736.0662\n",
      "Mini-batch EM epoch 64; train LL -734.5899\n",
      "Mini-batch EM epoch 65; train LL -733.127\n",
      "Mini-batch EM epoch 66; train LL -731.5597\n",
      "Mini-batch EM epoch 67; train LL -730.1395\n",
      "Mini-batch EM epoch 68; train LL -728.54944\n",
      "Mini-batch EM epoch 69; train LL -727.0399\n",
      "Mini-batch EM epoch 70; train LL -725.6504\n",
      "Mini-batch EM epoch 71; train LL -723.9792\n",
      "Mini-batch EM epoch 72; train LL -722.5363\n",
      "Mini-batch EM epoch 73; train LL -721.0183\n",
      "Mini-batch EM epoch 74; train LL -719.4697\n",
      "Mini-batch EM epoch 75; train LL -717.9531\n",
      "Mini-batch EM epoch 76; train LL -716.393\n",
      "Mini-batch EM epoch 77; train LL -714.79706\n",
      "Mini-batch EM epoch 78; train LL -713.22473\n",
      "Mini-batch EM epoch 79; train LL -711.7124\n",
      "Mini-batch EM epoch 80; train LL -710.18054\n",
      "Mini-batch EM epoch 81; train LL -708.5523\n",
      "Mini-batch EM epoch 82; train LL -706.86383\n",
      "Mini-batch EM epoch 83; train LL -705.29834\n",
      "Mini-batch EM epoch 84; train LL -703.6693\n",
      "Mini-batch EM epoch 85; train LL -701.9575\n",
      "Mini-batch EM epoch 86; train LL -700.2834\n",
      "Mini-batch EM epoch 87; train LL -698.669\n",
      "Mini-batch EM epoch 88; train LL -697.04425\n",
      "Mini-batch EM epoch 89; train LL -695.29004\n",
      "Mini-batch EM epoch 90; train LL -693.5637\n",
      "Mini-batch EM epoch 91; train LL -691.8745\n",
      "Mini-batch EM epoch 92; train LL -690.14813\n",
      "Mini-batch EM epoch 93; train LL -688.3997\n",
      "Mini-batch EM epoch 94; train LL -686.4603\n",
      "Mini-batch EM epoch 95; train LL -684.67267\n",
      "Mini-batch EM epoch 96; train LL -682.80597\n",
      "Mini-batch EM epoch 97; train LL -680.9514\n",
      "Mini-batch EM epoch 98; train LL -679.0704\n",
      "Mini-batch EM epoch 99; train LL -677.0779\n",
      "Mini-batch EM epoch 100; train LL -675.13196\n",
      "Mini-batch EM epoch 1; train LL -673.04047\n",
      "Mini-batch EM epoch 2; train LL -672.7693\n",
      "Mini-batch EM epoch 3; train LL -672.6608\n",
      "Mini-batch EM epoch 4; train LL -672.6099\n",
      "Mini-batch EM epoch 5; train LL -672.47644\n",
      "Mini-batch EM epoch 6; train LL -672.38\n",
      "Mini-batch EM epoch 7; train LL -672.31866\n",
      "Mini-batch EM epoch 8; train LL -672.2725\n",
      "Mini-batch EM epoch 9; train LL -672.13824\n",
      "Mini-batch EM epoch 10; train LL -672.0851\n",
      "Mini-batch EM epoch 11; train LL -671.92017\n",
      "Mini-batch EM epoch 12; train LL -671.9101\n",
      "Mini-batch EM epoch 13; train LL -671.90796\n",
      "Mini-batch EM epoch 14; train LL -671.79803\n",
      "Mini-batch EM epoch 15; train LL -671.77563\n",
      "Mini-batch EM epoch 16; train LL -671.66064\n",
      "Mini-batch EM epoch 17; train LL -671.6227\n",
      "Mini-batch EM epoch 18; train LL -671.5254\n",
      "Mini-batch EM epoch 19; train LL -671.47076\n",
      "Mini-batch EM epoch 20; train LL -671.434\n",
      "Mini-batch EM epoch 21; train LL -671.35815\n",
      "Mini-batch EM epoch 22; train LL -671.25977\n",
      "Mini-batch EM epoch 23; train LL -671.2192\n",
      "Mini-batch EM epoch 24; train LL -671.21075\n",
      "Mini-batch EM epoch 25; train LL -671.083\n",
      "Mini-batch EM epoch 26; train LL -671.074\n",
      "Mini-batch EM epoch 27; train LL -670.99664\n",
      "Mini-batch EM epoch 28; train LL -670.8925\n",
      "Mini-batch EM epoch 29; train LL -670.8121\n",
      "Mini-batch EM epoch 30; train LL -670.8006\n",
      "Mini-batch EM epoch 31; train LL -670.70764\n",
      "Mini-batch EM epoch 32; train LL -670.699\n",
      "Mini-batch EM epoch 33; train LL -670.6223\n",
      "Mini-batch EM epoch 34; train LL -670.59814\n",
      "Mini-batch EM epoch 35; train LL -670.4925\n",
      "Mini-batch EM epoch 36; train LL -670.4799\n",
      "Mini-batch EM epoch 37; train LL -670.4168\n",
      "Mini-batch EM epoch 38; train LL -670.3482\n",
      "Mini-batch EM epoch 39; train LL -670.2504\n",
      "Mini-batch EM epoch 40; train LL -670.252\n",
      "Mini-batch EM epoch 41; train LL -670.0993\n",
      "Mini-batch EM epoch 42; train LL -670.0926\n",
      "Mini-batch EM epoch 43; train LL -670.03925\n",
      "Mini-batch EM epoch 44; train LL -670.0027\n",
      "Mini-batch EM epoch 45; train LL -669.92737\n",
      "Mini-batch EM epoch 46; train LL -669.8774\n",
      "Mini-batch EM epoch 47; train LL -669.7507\n",
      "Mini-batch EM epoch 48; train LL -669.7419\n",
      "Mini-batch EM epoch 49; train LL -669.66705\n",
      "Mini-batch EM epoch 50; train LL -669.63354\n",
      "Mini-batch EM epoch 51; train LL -669.57\n",
      "Mini-batch EM epoch 52; train LL -669.5108\n",
      "Mini-batch EM epoch 53; train LL -669.39923\n",
      "Mini-batch EM epoch 54; train LL -669.40186\n",
      "Mini-batch EM epoch 55; train LL -669.2904\n",
      "Mini-batch EM epoch 56; train LL -669.2254\n",
      "Mini-batch EM epoch 57; train LL -669.1532\n",
      "Mini-batch EM epoch 58; train LL -669.1742\n",
      "Mini-batch EM epoch 59; train LL -669.0507\n",
      "Mini-batch EM epoch 60; train LL -669.0698\n",
      "Mini-batch EM epoch 61; train LL -668.99146\n",
      "Mini-batch EM epoch 62; train LL -668.8685\n",
      "Mini-batch EM epoch 63; train LL -668.85724\n",
      "Mini-batch EM epoch 64; train LL -668.7655\n",
      "Mini-batch EM epoch 65; train LL -668.78644\n",
      "Mini-batch EM epoch 66; train LL -668.692\n",
      "Mini-batch EM epoch 67; train LL -668.6334\n",
      "Mini-batch EM epoch 68; train LL -668.5979\n",
      "Mini-batch EM epoch 69; train LL -668.4889\n",
      "Mini-batch EM epoch 70; train LL -668.4601\n",
      "Mini-batch EM epoch 71; train LL -668.4522\n",
      "Mini-batch EM epoch 72; train LL -668.3708\n",
      "Mini-batch EM epoch 73; train LL -668.3208\n",
      "Mini-batch EM epoch 74; train LL -668.296\n",
      "Mini-batch EM epoch 75; train LL -668.15155\n",
      "Mini-batch EM epoch 76; train LL -668.0755\n",
      "Mini-batch EM epoch 77; train LL -668.03644\n",
      "Mini-batch EM epoch 78; train LL -667.9749\n",
      "Mini-batch EM epoch 79; train LL -667.9722\n",
      "Mini-batch EM epoch 80; train LL -667.8833\n",
      "Mini-batch EM epoch 81; train LL -667.90424\n",
      "Mini-batch EM epoch 82; train LL -667.789\n",
      "Mini-batch EM epoch 83; train LL -667.67267\n",
      "Mini-batch EM epoch 84; train LL -667.6414\n",
      "Mini-batch EM epoch 85; train LL -667.617\n",
      "Mini-batch EM epoch 86; train LL -667.54034\n",
      "Mini-batch EM epoch 87; train LL -667.4674\n",
      "Mini-batch EM epoch 88; train LL -667.4473\n",
      "Mini-batch EM epoch 89; train LL -667.34827\n",
      "Mini-batch EM epoch 90; train LL -667.33136\n",
      "Mini-batch EM epoch 91; train LL -667.2719\n",
      "Mini-batch EM epoch 92; train LL -667.1554\n",
      "Mini-batch EM epoch 93; train LL -667.14386\n",
      "Mini-batch EM epoch 94; train LL -667.00854\n",
      "Mini-batch EM epoch 95; train LL -667.05054\n",
      "Mini-batch EM epoch 96; train LL -666.9408\n",
      "Mini-batch EM epoch 97; train LL -666.8571\n",
      "Mini-batch EM epoch 98; train LL -666.81586\n",
      "Mini-batch EM epoch 99; train LL -666.8362\n",
      "Mini-batch EM epoch 100; train LL -666.7353\n",
      "Mini-batch EM epoch 101; train LL -666.6548\n",
      "Mini-batch EM epoch 102; train LL -666.64246\n",
      "Mini-batch EM epoch 103; train LL -666.5665\n",
      "Mini-batch EM epoch 104; train LL -666.5211\n",
      "Mini-batch EM epoch 105; train LL -666.4358\n",
      "Mini-batch EM epoch 106; train LL -666.3165\n",
      "Mini-batch EM epoch 107; train LL -666.3486\n",
      "Mini-batch EM epoch 108; train LL -666.27057\n",
      "Mini-batch EM epoch 109; train LL -666.16455\n",
      "Mini-batch EM epoch 110; train LL -666.19196\n",
      "Mini-batch EM epoch 111; train LL -666.10175\n",
      "Mini-batch EM epoch 112; train LL -666.0201\n",
      "Mini-batch EM epoch 113; train LL -665.9418\n",
      "Mini-batch EM epoch 114; train LL -665.8912\n",
      "Mini-batch EM epoch 115; train LL -665.9095\n",
      "Mini-batch EM epoch 116; train LL -665.767\n",
      "Mini-batch EM epoch 117; train LL -665.7385\n",
      "Mini-batch EM epoch 118; train LL -665.6942\n",
      "Mini-batch EM epoch 119; train LL -665.6117\n",
      "Mini-batch EM epoch 120; train LL -665.59924\n",
      "Mini-batch EM epoch 121; train LL -665.4585\n",
      "Mini-batch EM epoch 122; train LL -665.4855\n",
      "Mini-batch EM epoch 123; train LL -665.3779\n",
      "Mini-batch EM epoch 124; train LL -665.3056\n",
      "Mini-batch EM epoch 125; train LL -665.24615\n",
      "Mini-batch EM epoch 126; train LL -665.2419\n",
      "Mini-batch EM epoch 127; train LL -665.131\n",
      "Mini-batch EM epoch 128; train LL -665.0167\n",
      "Mini-batch EM epoch 129; train LL -665.0335\n",
      "Mini-batch EM epoch 130; train LL -664.90814\n",
      "Mini-batch EM epoch 131; train LL -664.946\n",
      "Mini-batch EM epoch 132; train LL -664.8144\n",
      "Mini-batch EM epoch 133; train LL -664.7832\n",
      "Mini-batch EM epoch 134; train LL -664.7778\n",
      "Mini-batch EM epoch 135; train LL -664.69775\n",
      "Mini-batch EM epoch 136; train LL -664.6088\n",
      "Mini-batch EM epoch 137; train LL -664.51965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch EM epoch 138; train LL -664.47217\n",
      "Mini-batch EM epoch 139; train LL -664.4386\n",
      "Mini-batch EM epoch 140; train LL -664.33105\n",
      "Mini-batch EM epoch 141; train LL -664.3288\n",
      "Mini-batch EM epoch 142; train LL -664.2715\n",
      "Mini-batch EM epoch 143; train LL -664.1785\n",
      "Mini-batch EM epoch 144; train LL -664.0954\n",
      "Mini-batch EM epoch 145; train LL -664.043\n",
      "Mini-batch EM epoch 146; train LL -664.0289\n",
      "Mini-batch EM epoch 147; train LL -664.0166\n",
      "Mini-batch EM epoch 148; train LL -663.8913\n",
      "Mini-batch EM epoch 149; train LL -663.7863\n",
      "Mini-batch EM epoch 150; train LL -663.75977\n",
      "Mini-batch EM epoch 151; train LL -663.7384\n",
      "Mini-batch EM epoch 152; train LL -663.62286\n",
      "Mini-batch EM epoch 153; train LL -663.56195\n",
      "Mini-batch EM epoch 154; train LL -663.57666\n",
      "Mini-batch EM epoch 155; train LL -663.42114\n",
      "Mini-batch EM epoch 156; train LL -663.4205\n",
      "Mini-batch EM epoch 157; train LL -663.3667\n",
      "Mini-batch EM epoch 158; train LL -663.3468\n",
      "Mini-batch EM epoch 159; train LL -663.29236\n",
      "Mini-batch EM epoch 160; train LL -663.14703\n",
      "Mini-batch EM epoch 161; train LL -663.0504\n",
      "Mini-batch EM epoch 162; train LL -663.041\n",
      "Mini-batch EM epoch 163; train LL -663.03357\n",
      "Mini-batch EM epoch 164; train LL -662.9096\n",
      "Mini-batch EM epoch 165; train LL -662.86743\n",
      "Mini-batch EM epoch 166; train LL -662.8014\n",
      "Mini-batch EM epoch 167; train LL -662.82465\n",
      "Mini-batch EM epoch 168; train LL -662.6477\n",
      "Mini-batch EM epoch 169; train LL -662.5829\n",
      "Mini-batch EM epoch 170; train LL -662.5623\n",
      "Mini-batch EM epoch 171; train LL -662.4887\n",
      "Mini-batch EM epoch 172; train LL -662.442\n",
      "Mini-batch EM epoch 173; train LL -662.3502\n",
      "Mini-batch EM epoch 174; train LL -662.3009\n",
      "Mini-batch EM epoch 175; train LL -662.21576\n",
      "Mini-batch EM epoch 176; train LL -662.15436\n",
      "Mini-batch EM epoch 177; train LL -662.1158\n",
      "Mini-batch EM epoch 178; train LL -662.0265\n",
      "Mini-batch EM epoch 179; train LL -662.00525\n",
      "Mini-batch EM epoch 180; train LL -661.90466\n",
      "Mini-batch EM epoch 181; train LL -661.9061\n",
      "Mini-batch EM epoch 182; train LL -661.8156\n",
      "Mini-batch EM epoch 183; train LL -661.7691\n",
      "Mini-batch EM epoch 184; train LL -661.66907\n",
      "Mini-batch EM epoch 185; train LL -661.6313\n",
      "Mini-batch EM epoch 186; train LL -661.5635\n",
      "Mini-batch EM epoch 187; train LL -661.4828\n",
      "Mini-batch EM epoch 188; train LL -661.4729\n",
      "Mini-batch EM epoch 189; train LL -661.40753\n",
      "Mini-batch EM epoch 190; train LL -661.2906\n",
      "Mini-batch EM epoch 191; train LL -661.2364\n",
      "Mini-batch EM epoch 192; train LL -661.1454\n",
      "Mini-batch EM epoch 193; train LL -661.1652\n",
      "Mini-batch EM epoch 194; train LL -661.0696\n",
      "Mini-batch EM epoch 195; train LL -660.95593\n",
      "Mini-batch EM epoch 196; train LL -660.9128\n",
      "Mini-batch EM epoch 197; train LL -660.84924\n",
      "Mini-batch EM epoch 198; train LL -660.8239\n",
      "Mini-batch EM epoch 199; train LL -660.79144\n",
      "Mini-batch EM epoch 200; train LL -660.7438\n",
      "Mini-batch EM epoch 201; train LL -660.5637\n",
      "Mini-batch EM epoch 202; train LL -660.60535\n",
      "Mini-batch EM epoch 203; train LL -660.5091\n",
      "Mini-batch EM epoch 204; train LL -660.4774\n",
      "Mini-batch EM epoch 205; train LL -660.32214\n",
      "Mini-batch EM epoch 206; train LL -660.2985\n",
      "Mini-batch EM epoch 207; train LL -660.24567\n",
      "Mini-batch EM epoch 208; train LL -660.16907\n",
      "Mini-batch EM epoch 209; train LL -660.09033\n",
      "Mini-batch EM epoch 210; train LL -660.0904\n",
      "Mini-batch EM epoch 211; train LL -659.9686\n",
      "Mini-batch EM epoch 212; train LL -659.89996\n",
      "Mini-batch EM epoch 213; train LL -659.8906\n",
      "Mini-batch EM epoch 214; train LL -659.72363\n",
      "Mini-batch EM epoch 215; train LL -659.7093\n",
      "Mini-batch EM epoch 216; train LL -659.66754\n",
      "Mini-batch EM epoch 217; train LL -659.6377\n",
      "Mini-batch EM epoch 218; train LL -659.4793\n",
      "Mini-batch EM epoch 219; train LL -659.4547\n",
      "Mini-batch EM epoch 220; train LL -659.3284\n",
      "Mini-batch EM epoch 221; train LL -659.3675\n",
      "Mini-batch EM epoch 222; train LL -659.2421\n",
      "Mini-batch EM epoch 223; train LL -659.1859\n",
      "Mini-batch EM epoch 224; train LL -659.1257\n",
      "Mini-batch EM epoch 225; train LL -658.9865\n",
      "Mini-batch EM epoch 226; train LL -658.97034\n",
      "Mini-batch EM epoch 227; train LL -658.901\n",
      "Mini-batch EM epoch 228; train LL -658.8508\n",
      "Mini-batch EM epoch 229; train LL -658.7841\n",
      "Mini-batch EM epoch 230; train LL -658.7464\n",
      "Mini-batch EM epoch 231; train LL -658.6696\n",
      "Mini-batch EM epoch 232; train LL -658.57306\n",
      "Mini-batch EM epoch 233; train LL -658.5016\n",
      "Mini-batch EM epoch 234; train LL -658.443\n",
      "Mini-batch EM epoch 235; train LL -658.3628\n",
      "Mini-batch EM epoch 236; train LL -658.32513\n",
      "Mini-batch EM epoch 237; train LL -658.25385\n",
      "Mini-batch EM epoch 238; train LL -658.1425\n",
      "Mini-batch EM epoch 239; train LL -658.1405\n",
      "Mini-batch EM epoch 240; train LL -658.0968\n",
      "Mini-batch EM epoch 241; train LL -657.9701\n",
      "Mini-batch EM epoch 242; train LL -657.8741\n",
      "Mini-batch EM epoch 243; train LL -657.80286\n",
      "Mini-batch EM epoch 244; train LL -657.7305\n",
      "Mini-batch EM epoch 245; train LL -657.7018\n",
      "Mini-batch EM epoch 246; train LL -657.6294\n",
      "Mini-batch EM epoch 247; train LL -657.6357\n",
      "Mini-batch EM epoch 248; train LL -657.5632\n",
      "Mini-batch EM epoch 249; train LL -657.4784\n",
      "Mini-batch EM epoch 250; train LL -657.38477\n",
      "Full-batch EM epoch 1; train LL -645.4794\n",
      "424.943130 seconds (327.08 M allocations: 14.098 GiB, 0.52% gc time, 3.14% compilation time)\n"
     ]
    }
   ],
   "source": [
    "num_epochs1       = 100\n",
    "num_epochs2       = 250\n",
    "num_epochs3       = 1\n",
    "batch_size        = 512\n",
    "pseudocount       = 0.1\n",
    "param_inertia1    = 0.1 \n",
    "param_inertia2    = 0.9\n",
    "param_inertia3    = 0.95\n",
    "\n",
    "@time begin\n",
    "    mini_batch_em(bpc, mnist_train_gpu, num_epochs1; batch_size, pseudocount, \n",
    "                  param_inertia = param_inertia1, param_inertia_end = param_inertia2);\n",
    "    mini_batch_em(bpc, mnist_train_gpu, num_epochs2; batch_size, pseudocount, \n",
    "                  param_inertia = param_inertia2, param_inertia_end = param_inertia3);\n",
    "    full_batch_em(bpc, mnist_train_gpu, num_epochs3; batch_size, pseudocount);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439dbc3",
   "metadata": {},
   "source": [
    "Now we evaluate the trained PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d004fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_ll: -632.69055\n",
      "Train_bpd: 1.1642595936712987\n",
      "Test LL: -652.6873\n",
      "Test_bpd: 1.2010570858863057\n"
     ]
    }
   ],
   "source": [
    "train_ll = loglikelihood(bpc, mnist_train_gpu; batch_size)\n",
    "test_ll = loglikelihood(bpc, mnist_test_gpu; batch_size)\n",
    "train_bpd = -train_ll / log(2.0) / 28^2\n",
    "test_bpd = -test_ll / log(2.0) / 28^2\n",
    "println(\"Train_ll: $(train_ll)\\nTrain_bpd: $(train_bpd)\\nTest LL: $(test_ll)\\nTest_bpd: $(test_bpd)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e97ac",
   "metadata": {},
   "source": [
    "Finally, we copy back the learned parameters from the bit circuit `bpc` to the original PC `pc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcc4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_parameters(bpc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
