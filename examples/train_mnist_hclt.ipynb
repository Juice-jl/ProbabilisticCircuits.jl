{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460fe332",
   "metadata": {},
   "source": [
    "ProbabilisticCircuits.jl offers various parameter and structure learning algorithms for PCs. In this example, we will demonstrate how to generate a particular PC model termed Hidden Chow-Liu Tree (HCLT) and use it to learn a state-of-the-art generative model on MNIST. \n",
    "\n",
    "We start by importing ProbabilisticCircuits.jl and other required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb49152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ProbabilisticCircuits [2396afbe-23d7-11ea-1e05-f1aa98e17a44]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    }
   ],
   "source": [
    "using ProbabilisticCircuits\n",
    "using MLDatasets\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8eaef",
   "metadata": {},
   "source": [
    "We first load the MNIST dataset from MLDatasets.jl and move them to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e53a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset summary:\n",
      " - Number of training examples: 60000\n",
      " - Number of test examples: 10000\n",
      " - Number of features: 784\n"
     ]
    }
   ],
   "source": [
    "mnist_train_cpu = collect(transpose(reshape(MNIST.traintensor(UInt8), 28*28, :)))\n",
    "mnist_test_cpu = collect(transpose(reshape(MNIST.testtensor(UInt8), 28*28, :)))\n",
    "mnist_train_gpu = cu(mnist_train_cpu)\n",
    "mnist_test_gpu = cu(mnist_test_cpu)\n",
    "println(\"Dataset summary:\\n - Number of training examples: $(size(mnist_train_cpu, 1))\\n - Number of test examples: $(size(mnist_test_cpu, 1))\\n - Number of features: $(size(mnist_train_cpu, 2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a6a7f",
   "metadata": {},
   "source": [
    "We move on to generate the HCLT structure. `hclt` constructs a smooth and structured-decomposable PC whose structure depends on the input samples. Specifically, it computes the pairwise mutual information (MI) between the MNIST features (i.e., pixels), and use the pairwise MI matrix to determine the PC structure, such that highly correlated features are placed \"closer\" in the PC to facilitate learning. In the following, `bits` is the number of bits to truncate to speedup the pairwise MI computation, and `latents` specifies the size of the generated HCLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47d108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HCLT structure with 32 latents... \n",
      " 24.339145 seconds (81.65 M allocations: 6.668 GiB, 5.43% gc time, 49.14% compilation time)\n",
      "Number of free parameters: 6980767\n"
     ]
    }
   ],
   "source": [
    "bits = 4\n",
    "latents = 32\n",
    "println(\"Generating HCLT structure with $latents latents... \");\n",
    "trunc_train = cu(mnist_train_cpu .÷ 2^bits)\n",
    "@time pc = hclt(trunc_train, latents; num_cats = 256, pseudocount = 0.1, input_type = Categorical)\n",
    "init_parameters(pc; perturbation = 0.4)\n",
    "println(\"Number of free parameters: $(num_parameters(pc))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81f93",
   "metadata": {},
   "source": [
    "To facilitate efficient parameter learning on GPUs, we first convert `pc` into an equivalent GPU-friendly low-level representation termed bits-circuit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02928e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving pc to GPU...   1.961610 seconds (17.83 M CPU allocations: 1.218 GiB, 7.18% gc time) (7 GPU allocations: 76.784 MiB, 0.00% memmgmt time)\n"
     ]
    }
   ],
   "source": [
    "print(\"Moving pc to GPU... \")\n",
    "CUDA.@time bpc = CuBitsProbCircuit(pc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec677c",
   "metadata": {},
   "source": [
    "We are now ready to train the parameters of the PC. This is done by calling the high-level API `mini_batch_em`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d51ee09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch EM iter 1; train LL -892.6227\n",
      "Mini-batch EM iter 2; train LL -824.05615\n",
      "Mini-batch EM iter 3; train LL -817.87933\n",
      "Mini-batch EM iter 4; train LL -814.094\n",
      "Mini-batch EM iter 5; train LL -810.924\n",
      "Mini-batch EM iter 6; train LL -808.42163\n",
      "Mini-batch EM iter 7; train LL -806.13763\n",
      "Mini-batch EM iter 8; train LL -804.1805\n",
      "Mini-batch EM iter 9; train LL -802.2172\n",
      "Mini-batch EM iter 10; train LL -800.28815\n",
      "Mini-batch EM iter 11; train LL -798.7425\n",
      "Mini-batch EM iter 12; train LL -797.2427\n",
      "Mini-batch EM iter 13; train LL -795.4564\n",
      "Mini-batch EM iter 14; train LL -794.0235\n",
      "Mini-batch EM iter 15; train LL -792.53564\n",
      "Mini-batch EM iter 16; train LL -791.11176\n",
      "Mini-batch EM iter 17; train LL -789.676\n",
      "Mini-batch EM iter 18; train LL -788.25336\n",
      "Mini-batch EM iter 19; train LL -786.91315\n",
      "Mini-batch EM iter 20; train LL -785.5561\n",
      "Mini-batch EM iter 21; train LL -784.1423\n",
      "Mini-batch EM iter 22; train LL -782.6588\n",
      "Mini-batch EM iter 23; train LL -781.3976\n",
      "Mini-batch EM iter 24; train LL -780.1783\n",
      "Mini-batch EM iter 25; train LL -778.65137\n",
      "Mini-batch EM iter 26; train LL -777.3741\n",
      "Mini-batch EM iter 27; train LL -775.933\n",
      "Mini-batch EM iter 28; train LL -774.78534\n",
      "Mini-batch EM iter 29; train LL -773.4646\n",
      "Mini-batch EM iter 30; train LL -772.0579\n",
      "Mini-batch EM iter 31; train LL -770.90625\n",
      "Mini-batch EM iter 32; train LL -769.5175\n",
      "Mini-batch EM iter 33; train LL -768.2782\n",
      "Mini-batch EM iter 34; train LL -766.8623\n",
      "Mini-batch EM iter 35; train LL -765.6913\n",
      "Mini-batch EM iter 36; train LL -764.2847\n",
      "Mini-batch EM iter 37; train LL -763.2565\n",
      "Mini-batch EM iter 38; train LL -761.74304\n",
      "Mini-batch EM iter 39; train LL -760.5317\n",
      "Mini-batch EM iter 40; train LL -759.187\n",
      "Mini-batch EM iter 41; train LL -757.90717\n",
      "Mini-batch EM iter 42; train LL -756.7233\n",
      "Mini-batch EM iter 43; train LL -755.36554\n",
      "Mini-batch EM iter 44; train LL -754.17957\n",
      "Mini-batch EM iter 45; train LL -752.94415\n",
      "Mini-batch EM iter 46; train LL -751.5734\n",
      "Mini-batch EM iter 47; train LL -750.2384\n",
      "Mini-batch EM iter 48; train LL -749.0347\n",
      "Mini-batch EM iter 49; train LL -747.72144\n",
      "Mini-batch EM iter 50; train LL -746.5221\n",
      "Mini-batch EM iter 51; train LL -745.32184\n",
      "Mini-batch EM iter 52; train LL -743.96826\n",
      "Mini-batch EM iter 53; train LL -742.77765\n",
      "Mini-batch EM iter 54; train LL -741.47327\n",
      "Mini-batch EM iter 55; train LL -740.22205\n",
      "Mini-batch EM iter 56; train LL -738.9783\n",
      "Mini-batch EM iter 57; train LL -737.70667\n",
      "Mini-batch EM iter 58; train LL -736.277\n",
      "Mini-batch EM iter 59; train LL -735.07837\n",
      "Mini-batch EM iter 60; train LL -733.6995\n",
      "Mini-batch EM iter 61; train LL -732.4089\n",
      "Mini-batch EM iter 62; train LL -731.1374\n",
      "Mini-batch EM iter 63; train LL -729.8065\n",
      "Mini-batch EM iter 64; train LL -728.5193\n",
      "Mini-batch EM iter 65; train LL -727.16406\n",
      "Mini-batch EM iter 66; train LL -725.8736\n",
      "Mini-batch EM iter 67; train LL -724.575\n",
      "Mini-batch EM iter 68; train LL -723.3353\n",
      "Mini-batch EM iter 69; train LL -721.8951\n",
      "Mini-batch EM iter 70; train LL -720.5368\n",
      "Mini-batch EM iter 71; train LL -719.1165\n",
      "Mini-batch EM iter 72; train LL -717.7905\n",
      "Mini-batch EM iter 73; train LL -716.4972\n",
      "Mini-batch EM iter 74; train LL -715.11206\n",
      "Mini-batch EM iter 75; train LL -713.6472\n",
      "Mini-batch EM iter 76; train LL -712.2732\n",
      "Mini-batch EM iter 77; train LL -710.9582\n",
      "Mini-batch EM iter 78; train LL -709.6025\n",
      "Mini-batch EM iter 79; train LL -708.0827\n",
      "Mini-batch EM iter 80; train LL -706.6898\n",
      "Mini-batch EM iter 81; train LL -705.29486\n",
      "Mini-batch EM iter 82; train LL -703.8747\n",
      "Mini-batch EM iter 83; train LL -702.4367\n",
      "Mini-batch EM iter 84; train LL -701.0705\n",
      "Mini-batch EM iter 85; train LL -699.4644\n",
      "Mini-batch EM iter 86; train LL -697.99225\n",
      "Mini-batch EM iter 87; train LL -696.6085\n",
      "Mini-batch EM iter 88; train LL -694.9879\n",
      "Mini-batch EM iter 89; train LL -693.47675\n",
      "Mini-batch EM iter 90; train LL -692.0093\n",
      "Mini-batch EM iter 91; train LL -690.49786\n",
      "Mini-batch EM iter 92; train LL -688.9729\n",
      "Mini-batch EM iter 93; train LL -687.3101\n",
      "Mini-batch EM iter 94; train LL -685.7062\n",
      "Mini-batch EM iter 95; train LL -684.00214\n",
      "Mini-batch EM iter 96; train LL -682.4406\n",
      "Mini-batch EM iter 97; train LL -680.7632\n",
      "Mini-batch EM iter 98; train LL -679.1242\n",
      "Mini-batch EM iter 99; train LL -677.32745\n",
      "Mini-batch EM iter 100; train LL -675.5845\n",
      "137.179135 seconds (124.39 M allocations: 5.612 GiB, 0.81% gc time, 7.05% compilation time)\n"
     ]
    }
   ],
   "source": [
    "num_epochs        = 100\n",
    "batch_size        = 512\n",
    "pseudocount       = 0.1\n",
    "param_inertia     = 0.2 # Equivalent to 1-[minibatch stepsize]\n",
    "param_inertia_end = 0.9 # If specified, param_inertia will be annealed linearly during training\n",
    "\n",
    "@time mini_batch_em(bpc, mnist_train_gpu, num_epochs; batch_size, pseudocount, \n",
    "                    param_inertia, param_inertia_end);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439dbc3",
   "metadata": {},
   "source": [
    "Now we evaluate the trained PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d004fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_ll: -663.66864\n",
      "Test LL: -672.6025\n"
     ]
    }
   ],
   "source": [
    "train_ll = loglikelihood(bpc, mnist_train_gpu; batch_size)\n",
    "test_ll = loglikelihood(bpc, mnist_test_gpu; batch_size)\n",
    "println(\"Train_ll: $(train_ll)\\nTest LL: $(test_ll)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e97ac",
   "metadata": {},
   "source": [
    "Finally, we want to copy back the learned parameters from the bit circuit `bpc` to the original PC `pc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcc4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_parameters(bpc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
