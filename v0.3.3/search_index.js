var documenterSearchIndex = {"docs":
[{"location":"api/internals/utils/#api-internal-utils","page":"Utils","title":"Utils","text":"","category":"section"},{"location":"api/internals/utils/","page":"Utils","title":"Utils","text":"Modules = [ProbabilisticCircuits.Utils]","category":"page"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils","page":"Utils","title":"ProbabilisticCircuits.Utils","text":"Module with general utilities and missing standard library features  that could be useful in any Julia project\n\n\n\n\n\n","category":"module"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.DisCache","page":"Utils","title":"ProbabilisticCircuits.Utils.DisCache","text":"Cache pairwise / marginal distribution for all variables in one dataset\n\n\n\n\n\n","category":"type"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.bernoulli_partition-Tuple{Vector{Int64}, Float64}","page":"Utils","title":"ProbabilisticCircuits.Utils.bernoulli_partition","text":"Split X into two partitions A and B, where A is a Bernoulli sample of each element in X with probability p and B=X∖A. Guarantees at least one element in A and B.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.generate_all-Tuple{Vector{T} where T}","page":"Utils","title":"ProbabilisticCircuits.Utils.generate_all","text":"Given some missing values generates all possible fillings\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.generate_data_all-Tuple{Int64}","page":"Utils","title":"ProbabilisticCircuits.Utils.generate_data_all","text":"Generates all possible binary configurations of size N\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.kfold-Tuple{Int64, Int64}","page":"Utils","title":"ProbabilisticCircuits.Utils.kfold","text":"Returns a(n index) partitioning a la k-fold.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.mutual_information","page":"Utils","title":"ProbabilisticCircuits.Utils.mutual_information","text":"Calculate mutual information of given bit matrix bm, example weights w, and smoothing pseudocount α\n\n\n\n\n\n","category":"function"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.one_hot_encode-Union{Tuple{T}, Tuple{Matrix{T}, Vector{T}}} where T","page":"Utils","title":"ProbabilisticCircuits.Utils.one_hot_encode","text":"One-hot encode data (2-D Array) based on categories (1-D Array) Each row of the return value is a concatenation of one-hot encoding of elements of the same row in data Assumption: both input arrays have elements of same type\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.pairwise_marginals","page":"Utils","title":"ProbabilisticCircuits.Utils.pairwise_marginals","text":"Compute an array giving all pairwise marginals estimated on empirical (weighted) data\n\n\n\n\n\n","category":"function"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.sample_vtree-Tuple{Int64, Float64}","page":"Utils","title":"ProbabilisticCircuits.Utils.sample_vtree","text":"Samples a Vtree with a right bias of p. If p<0, then uniformly sample vtrees.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/utils/#ProbabilisticCircuits.Utils.set_mutual_information-Tuple{Matrix{T} where T, AbstractVector{var\"#s23\"} where var\"#s23\"<:(AbstractVector{T} where T)}","page":"Utils","title":"ProbabilisticCircuits.Utils.set_mutual_information","text":"Calculate set mutual information\n\n\n\n\n\n","category":"method"},{"location":"api/types/#api-types","page":"Type Trees","title":"Type Trees","text":"","category":"section"},{"location":"api/types/","page":"Type Trees","title":"Type Trees","text":"The following code snippet provides an easy way to print the type tree of probabilistic circuits.","category":"page"},{"location":"api/types/","page":"Type Trees","title":"Type Trees","text":"using InteractiveUtils;\nusing ProbabilisticCircuits;\nusing AbstractTrees;\nAbstractTrees.children(x::Type) = subtypes(x);","category":"page"},{"location":"api/types/","page":"Type Trees","title":"Type Trees","text":"For example, we can see ProbabilisticCircuits.ProbCircuit's type tree.","category":"page"},{"location":"api/types/","page":"Type Trees","title":"Type Trees","text":"AbstractTrees.print_tree(ProbCircuit)","category":"page"},{"location":"api/internals/probabilistic_circuits/#api-internal-probabilistic","page":"Probabilistic Circuits","title":"Probabilistic Circuits","text":"","category":"section"},{"location":"api/internals/probabilistic_circuits/","page":"Probabilistic Circuits","title":"Probabilistic Circuits","text":"Modules = [ProbabilisticCircuits]","category":"page"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.BayesModelComb","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.BayesModelComb","text":"Bayesian Model Combination.\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.CLT","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.CLT","text":"Chow-Liu Tree\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.Ensemble","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.Ensemble","text":"Weighted ensemble of probabilistic circuits.\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.ParamBitCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.ParamBitCircuit","text":"A BitCircuit with parameters attached to the elements\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainMulNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainMulNode","text":"A probabilistic conjunction node (multiplication node)\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainProbCircuit","text":"Root of the plain probabilistic circuit node hierarchy\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainProbInnerNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainProbInnerNode","text":"A probabilistic inner node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainProbLeafNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainProbLeafNode","text":"A probabilistic leaf node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainProbLiteralNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainProbLiteralNode","text":"A probabilistic literal node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.PlainSumNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.PlainSumNode","text":"A probabilistic disjunction node (summation node)\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.ProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.ProbCircuit","text":"Root of the probabilistic circuit node hierarchy\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.ProbCircuit-Tuple","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.ProbCircuit","text":"Evaluate a probabilistic circuit as a function\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.RegionGraph","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.RegionGraph","text":"Root of region graph node hierarchy\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SamplingOpts","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SamplingOpts","text":"Options for SamplePSDD.\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedMulNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedMulNode","text":"A shared probabilistic multiplcation node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedProbCircuit","text":"Root of the shared probabilistic circuit node hierarchy\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedProbInnerNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedProbInnerNode","text":"A shared probabilistic inner node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedProbLeafNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedProbLeafNode","text":"A shared probabilistic leaf node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedProbLiteralNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedProbLiteralNode","text":"A shared probabilistic literal node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.SharedSumNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.SharedSumNode","text":"A shared probabilistic summation node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructMulNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructMulNode","text":"A plain structured probabilistic conjunction node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructProbCircuit","text":"Root of the plain structure probabilistic circuit node hierarchy\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructProbInnerNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructProbInnerNode","text":"A plain structured probabilistic inner node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructProbLeafNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructProbLeafNode","text":"A plain structured probabilistic leaf node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructProbLiteralNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructProbLiteralNode","text":"A plain structured probabilistic literal leaf node, representing the positive or negative literal of its variable\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.StructSumNode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.StructSumNode","text":"A plain structured probabilistic disjunction node\n\n\n\n\n\n","category":"type"},{"location":"api/internals/probabilistic_circuits/#Base.read-Union{Tuple{C}, Tuple{AbstractString, Type{C}}} where C<:ProbCircuit","page":"Probabilistic Circuits","title":"Base.read","text":"Base.read(file::AbstractString, ::Type{C}) where C <: ProbCircuit\n\nReads circuit from file; uses extension to detect format type, for example \".psdd\" for PSDDs.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#Base.write-Tuple{AbstractString, ProbCircuit}","page":"Probabilistic Circuits","title":"Base.write","text":"Base.write(file::AbstractString, circuit::ProbCircuit)\n\nWrites circuit to file; uses file name extention to detect file format.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#Base.write-Tuple{Tuple{AbstractString, AbstractString}, StructProbCircuit}","page":"Probabilistic Circuits","title":"Base.write","text":"Base.write(files::Tuple{AbstractString,AbstractString}, circuit::StructProbCircuit)\n\nSaves circuit and vtree to file.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#DirectedAcyclicGraphs.print_tree-Tuple{MetaGraphs.MetaDiGraph}","page":"Probabilistic Circuits","title":"DirectedAcyclicGraphs.print_tree","text":"Print edges and vertices of a ChowLiu tree\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#LogicCircuits.vtree-Tuple{StructProbCircuit}","page":"Probabilistic Circuits","title":"LogicCircuits.vtree","text":"Get the vtree corresponding to the argument, or nothing if the node has no vtree\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.EVI","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.EVI","text":"EVI(pc, data)\n\nComputes the log likelihood data given full evidence. Outputs logp(x) for each datapoint.\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.MAP","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.MAP","text":"Maximum a-posteriori queries\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.MAR","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.MAR","text":"MAR(pc, data) = marginal\n\nComputes Marginal log likelhood of data. MAR is juat an alias for marginal. See docs for marginal for more details.\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.MST-Union{Tuple{T}, Tuple{SimpleWeightedGraphs.SimpleWeightedGraph, Matrix{T}, Vector{SimpleWeightedGraphs.SimpleWeightedEdge}, Vector{SimpleWeightedGraphs.SimpleWeightedEdge}}} where T<:AbstractFloat","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.MST","text":"Compute the Minimum Spanning Tree (MST) of graph g with weights weights, with  constraints such that included_edges should be included while excluded_edges   should be excluded.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.apply_entropy_reg_cpu-Tuple{LogicCircuits.BitCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.apply_entropy_reg_cpu","text":"apply_entropy_reg_cpu(bc::BitCircuit; log_params::Vector{Float64}, edge_counts::Vector{Float64}, total_data_counts::Float64, pseudocount::Float64 = 0.1, entropy_reg::Float64 = 0.0)::Vector{Float64}\n\nAdd entropy regularization to a deterministic (see LogicCircuits.isdeterministic) probabilistic circuit. alpha is a hyperparameter that balances the weights between the likelihood and the  entropy: \u0007rgmax{\theta} L(\theta) = llmean(\theta) + alpha * entropy(\theta).\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.apply_entropy_reg_gpu-Tuple{LogicCircuits.BitCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.apply_entropy_reg_gpu","text":"apply_entropy_reg_gpu(bc::BitCircuit; log_params, edge_counts, total_data_counts::Float64, pseudocount::Float64 = 0.1, entropy_reg::Float64 = 0.0)\n\nAdd entropy regularization to a deterministic (see LogicCircuits.isdeterministic) probabilistic circuit. alpha is a hyperparameter that balances the weights between the likelihood and the  entropy: \u0007rgmax{\theta} L(\theta) = llmean(\theta) + alpha * entropy(\theta).\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.balance_sum-Tuple{Vector{ProbCircuit}, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.balance_sum","text":"Makes sure the sum nodes does not have too many children. Makes balanced sums of sums to reduce children count.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.balanced_fully_factorized-Tuple{Vector{var\"#s189\"} where var\"#s189\"<:LogicCircuits.Vtree}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.balanced_fully_factorized","text":"Makes sure input nodes don't have too many parents. Makes a dummy sum node for each input per partition. Then nodes corresponding to the partition use the dummy node as their children instead of the input node. This way instead of numnodesroot * numnodesleaf, we would have numnodesroot parents nodes.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.bdd_vtree!-Tuple{AbstractVector{var\"#s191\"} where var\"#s191\"<:Integer}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.bdd_vtree!","text":"Returns an increasing sorted right-linear vtree.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.bmc_sample_psdd-Tuple{Integer, LogicCircuits.Bdd, Integer, DataFrames.DataFrame, Integer, Integer}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.bmc_sample_psdd","text":"Constructs a SamplePSDD BMC with q*t combinations, each with n models.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.bottom_up_traverse_node_seq-Tuple{MetaGraphs.MetaDiGraph}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.bottom_up_traverse_node_seq","text":"Return a vertex sequence to traverse the tree `g', where children are accessed before parent.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.chow_liu_tree-Tuple{Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.chow_liu_tree","text":"Compute the Chow-Liu Tree given a binary dataset.  Automatically convert to categorical dataset if specified by num_vars' andnumcats'.  If `numtrees` is greater than 1, the algorithm returns the top-K maximum spanning trees  with respect to the pairwise_MI weights.  Reference: Listing all the minimum spanning trees in an undirected graph  http://www.nda.ac.jp/~yamada/paper/enum-mst.pdf\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.compile_canonical_literals-Tuple{UInt32, LogicCircuits.PlainVtreeLeafNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.compile_canonical_literals","text":"Construct literal nodes given variable var\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.compile_decision_node-Tuple{Vector{var\"#s189\"} where var\"#s189\"<:LogicCircuits.PlainStructLogicCircuit, Vector{var\"#s188\"} where var\"#s188\"<:LogicCircuits.PlainStructLogicCircuit, LogicCircuits.PlainVtreeInnerNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.compile_decision_node","text":"Construct decision nodes given primes and subs\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.compile_decision_nodes-Tuple{Vector{var\"#s186\"} where var\"#s186\"<:LogicCircuits.PlainStructLogicCircuit, Vector{var\"#s143\"} where var\"#s143\"<:LogicCircuits.PlainStructLogicCircuit, LogicCircuits.PlainVtreeInnerNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.compile_decision_nodes","text":"Construct decision nodes conditiond on different distribution\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.compile_sdd_from_clt-Tuple{MetaGraphs.MetaDiGraph, LogicCircuits.PlainVtree}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.compile_sdd_from_clt","text":"Compile a psdd circuit from clt and vtree\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.compile_true_nodes-Tuple{UInt32, LogicCircuits.PlainVtreeLeafNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.compile_true_nodes","text":"Construct true nodes given variable var\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.eFlow-NTuple{4, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.eFlow","text":"Pick the edge with maximum flow\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.eRand-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.eRand","text":"Pick the edge randomly\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.ensemble_sample_psdd-Tuple{Integer, LogicCircuits.Bdd, Int64, DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.ensemble_sample_psdd","text":"Creates an ensemble of n SamplePSDD-generated probabilistic circuits, with v the total number of variables in the data, ϕ the logic constraints, D the data to be learned and k the maximum number of primes to be sampled.\n\nKeyword arguments for sample_psdd are passed down. Optionally, the function takes keyword argument vtree_bias, which samples more left (value closer to 0.0) or right-leaning (value closer to 1.0) vtrees. If a negative value is given, sample uniformly distributed vtrees.\n\nWeights are computed by the given strategy. These can be any one of the following:     1. :likelihood for likelihood weighting;     2. :uniform for uniform weights;     3. :em for Expectation-Maximization;     4. :stacking for mixture model Stacking;\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters!","text":"estimate_parameters!(pc::ProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0)\n\nMaximum likelihood estimation of a ProbCircuit's parameters given data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters!-Tuple{SharedProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters!","text":"estimate_parameters!(spc::SharedProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0)\n\nMaximum likelihood estimation of a SharedProbCircuit's parameters given data.\n\nbagging support: If spc is a SharedProbCircuit and data is an array of DataFrames   with the same number of \"components\", learn each circuit with its corresponding    dataset.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_cpu-Tuple{LogicCircuits.BitCircuit, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_cpu","text":"estimate_parameters_cpu(bc::BitCircuit, data, pseudocount; weights = nothing, entropy_reg::Float64 = 0.0)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_em!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_em!","text":"estimate_parameters_em!(pc::ProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0, exp_update_factor::Float64 = 0.0, update_per_batch::Bool = false)\n\nOne epoch of Expectation maximization (EM) parameter learning for circuits. Useful when having missing data or non-deterministic circuits.\n\nentropy_reg: Entropy Regularization\nupdate_per_batch: Whether to update circuit paramters per batch (using the ParamBitCircuit's paramters)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_em_multi_epochs!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_em_multi_epochs!","text":"estimate_parameters_em_multi_epochs!(circuit::ProbCircuit, train_data)\n\nRuns multiple epochs of EM for circuit. It will run on CPU/GPU based on where train_data is.\n\nArguments: \n\ncircuit:\ntrain_data: training data. If want gpu, move to gpu before calling this to_gpu(train_data).\n\nKeyword arguments:\n\nvalid_data=nothing: validation data, should be same device as train_data \ntest_data=nothing: test data, data, should be same device as train_data \nentropy_reg::Float64 = 0.0: Entropy regularization\nexp_update_factor_start::Float64 = 0.1: \nexp_update_factor_end::Float64 = 0.9:\nem_warmup_iters=100 : number of EM Warm up iterations\nem_finetune_iters=100: number of EM fine tune iterations\nverbose=false: verbose or not\nverbose_log_rate=1: how often to print info\nsave_path=nothing: path of the file to save the partially trained circuit\nsave_rate=20: how often to save the circuit to file\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_em_per_batch!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_em_per_batch!","text":"estimate_parameters_em_per_batch!(pc::ProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0, exp_update_factor = 0.0)\n\nOne epoch of Expectation maximization (EM) parameter learning for circuits. Useful when having missing data or non-deterministic circuits.\n\nentropy_reg: Entropy Regularization\nupdate_per_batch: Whether to update circuit paramters per batch (using the ParamBitCircuit's paramters)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_gpu-Tuple{LogicCircuits.BitCircuit, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_gpu","text":"estimate_parameters_gpu(bc::BitCircuit, data, pseudocount; weights = nothing, entropy_reg::Float64 = 0.0)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_sgd!-Tuple{ParamBitCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_sgd!","text":"estimate_parameters_sgd!(pc::ProbCircuit, data; lr::Float64 = 0.01, reuse_values = nothing, reuse_flows = nothing, reuse = (nothing, nothing))\n\nOne epoch SGD to learn paramters of a ParamBitCircuit. It will run on CPU/GPU depending on where the data is located.\n\nlr: Learning Rate\npbc: The ParamBitCircuit\n'data': training data\n'resue_*`: Allows reusing of allocated memory to avoid extra memory allocation between epochs. If training for only one epoch don't need to specify them.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_parameters_sgd!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_parameters_sgd!","text":"estimate_parameters_sgd!(pc::ProbCircuit, data; lr::Float64 = 0.01, reuse_values = nothing, reuse_flows = nothing, reuse = (nothing, nothing))\n\nOne epoch SGD to learn paramters of the pc. It will run on CPU/GPU depending on where the data is located.\n\nlr: Learning Rate\npc: The ProbCircuit\n'data': training data\n'resue_*`: Allows reusing of allocated memory to avoid extra memory allocation between epochs. If training for only one epoch don't need to specify them.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.estimate_single_circuit_parameters!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.estimate_single_circuit_parameters!","text":"estimate_single_circuit_parameters!(pc::ProbCircuit, data; pseudocount::Float64, component_idx::Integer = 0, entropy_reg::Float64 = 0.0)\n\nMaximum likelihood estimation of a single circuit (for example, ProbCircuit or one component of SharedProbCircuit) parameters given data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.factorize_reuse-Tuple{Vector{Int64}, Dict{Int32, StructProbLiteralNode}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.factorize_reuse","text":"Returns a fully factorized circuit reusing leaf nodes from L.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.generate_from_bdd-Tuple{LogicCircuits.Bdd, Integer}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.generate_from_bdd","text":"Returns a structured probabilistic circuit compiled from a binary decision diagram.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.get_cpt-Tuple{Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.get_cpt","text":"Calculate CPT of child conditioned on parent from dis_cache\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.init_marginal-Tuple{Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.init_marginal","text":"init_marginal(data, reuse, num_nodes; Float=Float32)\n\nInitialize values from the data (data frames)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.init_marginal_cpu-Tuple{Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.init_marginal_cpu","text":"init_marginal_cpu(data, reuse, num_nodes; Float)\n\nInitialize values from the cpu data (data frames)    \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.init_marginal_gpu-Tuple{Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.init_marginal_gpu","text":"init_marginal_gpu(data, reuse, num_nodes; Float=Float32)\n\nInitialize values from the gpu data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.ismul-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.ismul","text":"Is the node a multiplication?\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.issum-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.issum","text":"Is the node a summation?\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.kl_divergence","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.kl_divergence","text":"Calculate KL divergence calculation for pcs that are not necessarily identical\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_bdd-Tuple{LogicCircuits.Bdd, DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_bdd","text":"Learns a structured probabilistic circuit consistent with a binary decision diagram ϕ.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_chow_liu_tree-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_chow_liu_tree","text":"learn a Chow-Liu tree from training set train_x, with Laplace smoothing factor α, specifying the tree root by clt_root return a CLT\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_chow_liu_tree_circuit-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_chow_liu_tree_circuit","text":"Learning from data a structured-decomposable circuit with several structure learning algorithms\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_circuit-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_circuit","text":"Learn structure of a single structured decomposable circuit\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_circuit_miss-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_circuit_miss","text":"Learn structure of a single structured decomposable circuit from missing data. \n\nMissing feature are denoted by missing.  Median Imputation is used by default for initial structure (set impute_method for other options).\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_circuit_mixture-Tuple{Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_circuit_mixture","text":"Given a circuit, learns a mixture of structure decomposable circuits based on that circuit\n\nlearn_circuit_mixture(pc, data; num_mix=5, pseudocount=1.0, maxiter=20)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_ensemble_em!-Union{Tuple{T}, Tuple{Ensemble{T}, DataFrames.DataFrame}} where T<:ProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_ensemble_em!","text":"Learns the weights of the Ensemble by Expectation-Maximization.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_ensemble_llw!-Union{Tuple{T}, Tuple{Ensemble{T}, DataFrames.DataFrame}} where T<:ProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_ensemble_llw!","text":"Learns the weights of the Ensemble by the likelihood value of data D.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_ensemble_stacking!-Union{Tuple{T}, Tuple{Ensemble{T}, DataFrames.DataFrame}} where T<:ProbCircuit","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_ensemble_stacking!","text":"Learns the weights of the Ensemble by Stacking, with k as the number of folds in k-fold.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_strudel-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_strudel","text":"learn_strudel(train_x; num_mix = 5, init_maxiter = 10, em_maxiter=20)\n\nLearn a mixture of circuits See \"Strudel: Learning Structured-Decomposable Probabilistic Circuits. arxiv.org/abs/2007.09331\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.learn_vtree_from_clt-Tuple{MetaGraphs.MetaDiGraph}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.learn_vtree_from_clt","text":"Learn a vtree from clt, with strategy (close to) linear or balanced\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.load_as_ensemble-Tuple{String}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.load_as_ensemble","text":"Loads an ensemble from disk.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.log_likelihood-Tuple{Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.log_likelihood","text":"log_likelihood(pc, data)\n\nCompute the likelihood of the PC given the data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.log_likelihood_avg-Tuple{Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.log_likelihood_avg","text":"log_likelihood_avg(pc, data)\n\nCompute the likelihood of the PC given the data, averaged over all instances in the data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.log_likelihood_per_instance","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.log_likelihood_per_instance","text":"Compute the likelihood of the PC given each individual instance in the data\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_child-NTuple{6, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_child","text":"Find the MAP child value and node id of a given decision node\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_prob-Tuple{ProbCircuit, Vararg{Union{Missing, Real}, N} where N}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_prob","text":"map_prob(root::ProbCircuit, data::Union{Real,Missing}...)\nmap_prob(root::ProbCircuit, data::Union{Vector{Union{Bool,Missing}},CuVector{UInt8}})\nmap_prob(circuit::ProbCircuit, data::DataFrame)\nmap_prob(circuit::ParamBitCircuit, data::DataFrame)\n\nThe most likely world probability that agrees with the provided data\"\n\nMissing values should be denoted by missing in the data.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_prob_all-Tuple{ProbCircuit, DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_prob_all","text":"The most likely world that agrees with the provided data for all nodes\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_prob_layers-Tuple{ParamBitCircuit, CUDA.CuArray{T, 2, B} where {T, B}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_prob_layers","text":"Compute marginals on the GPU\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_prob_layers-Tuple{ParamBitCircuit, Matrix{T} where T}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_prob_layers","text":"Compute MAP probabilities on the CPU (SIMD & multi-threaded)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.map_prob_layers_cuda-NTuple{5, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.map_prob_layers_cuda","text":"CUDA kernel for circuit evaluation\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal","text":"marginal(circuit::SharedProbCircuit, data::DataFrame, weights::Union{AbstractArray, Nothing} = nothing; component_idx = 0)::AbstractVector\n\nComputes marginals for one circuit in a SharedProbCircuit.\n\ncomponent_idx: index of the SharedProbCircuit component to comptue marginals on.\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal-Tuple{ProbCircuit, Vararg{Union{Missing, Real}, N} where N}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal","text":"marginal(root::ProbCircuit, data::Union{Real,Missing}...)\nmarginal(root::ProbCircuit, data::Union{Vector{Union{Bool,Missing}},CuVector{UInt8}})\nmarginal(circuit::ProbCircuit, data::DataFrame)\nmarginal(circuit::ParamBitCircuit, data::DataFrame)::AbstractVector\nmarginal(circuit::SharedProbCircuit, data::DataFrame, weights::Union{AbstractArray, Nothing}; component_idx)\n\nEvaluate marginals of the circuit bottom-up for given input(s).\n\nMissing values should be denoted by missing in the data.\n\nOutputs logp(x^o) for each data point. \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_all-Tuple{ProbCircuit, DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_all","text":"Evaluate the probabilistic circuit bottom-up for a given input and return the marginal probability value of all nodes\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_flows","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_flows","text":"Compute the marginal and flow of each node\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_flows_down","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_flows_down","text":"When marginals of nodes have already been computed, do a downward pass computing the marginal flows at each node\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_flows_down_layers-Tuple{ParamBitCircuit, CUDA.CuArray{T, 2, B} where {T, B}, CUDA.CuArray{T, 2, B} where {T, B}, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_flows_down_layers","text":"Pass marginal flows down the layers of a bit circuit on the GPU\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_flows_down_layers-Tuple{ParamBitCircuit, Matrix{T} where T, Matrix{T} where T, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_flows_down_layers","text":"Evaluate marginals of the layers of a parameter bit circuit on the CPU (SIMD & multi-threaded)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_flows_down_layers_cuda-Tuple{Any, Any, Any, Any, Any, Any, Any, Any, Any, Nothing}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_flows_down_layers_cuda","text":"CUDA kernel for passing marginal flows down circuit\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_layers-Tuple{ParamBitCircuit, CUDA.CuArray{T, 2, B} where {T, B}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_layers","text":"Compute marginals on the GPU\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_layers-Tuple{ParamBitCircuit, Matrix{T} where T}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_layers","text":"Compute marginals on the CPU (SIMD & multi-threaded)\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_layers_cuda-NTuple{5, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_layers_cuda","text":"CUDA kernel for circuit evaluation\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_log_likelihood-Tuple{Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_log_likelihood","text":"marginal_log_likelihood(pc, data)\nmarginal_log_likelihood(pc, data, weights::DataFrame)\nmarginal_log_likelihood(pc, data, weights::AbstractArray)\nmarginal_log_likelihood(pc, data::Vector{DataFrame})\n\nCompute the marginal likelihood of the PC given the data\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.marginal_log_likelihood_avg-Tuple{Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.marginal_log_likelihood_avg","text":"marginal_log_likelihood_avg(pc, data)\n\nCompute the marginal likelihood of the PC given the data, averaged over all instances in the data. If the data is weighted then does a weighted average.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.max_a_posteriori-Tuple{ProbCircuit, Vararg{Union{Missing, Bool}, N} where N}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.max_a_posteriori","text":"max_a_posteriori(root::ProbCircuit, data::Union{Bool,Missing}...)\nmax_a_posteriori(root::ProbCircuit, data::Union{Vector{<:Union{Bool,Missing}},CuVector{UInt8}})\nmax_a_posteriori(circuit::ProbCircuit, data::DataFrame)\nmax_a_posteriori(pbc::ParamBitCircuit, data; Float=Float32)\n\nEvaluate maximum a-posteriori state of the circuit for given input(s).\n\nOutputs the states, and the corresponding probabilities (in log domain).\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.metis_top_down-Tuple{DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.metis_top_down","text":"Metis top down method\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.mode","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.mode","text":"Mode of the distribution\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.mul_nodes-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.mul_nodes","text":"Get the list of multiplication nodes in a given circuit\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.multiply","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.multiply","text":"Multiply nodes into a single circuit\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.num_components-Tuple{SharedSumNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.num_components","text":"How many components are mixed together in this shared circuit?\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.num_parameters-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.num_parameters","text":"Count the number of parameters in the circuit\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.num_parameters_node-Tuple{PlainSumNode}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.num_parameters_node","text":"Count the number of parameters in the node\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.pairwise_MI-Tuple{DataFrames.DataFrame, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.pairwise_MI","text":"Compute pairwise Mutual Information given binary/categorical data.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.params-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.params","text":"Get the parameters associated with a sum node\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.parent_vector-Tuple{MetaGraphs.MetaDiGraph}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.parent_vector","text":"Get parent vector of a tree\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.parse_clt-Tuple{String}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.parse_clt","text":"Parse a clt from given file\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.pr_constraint","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.pr_constraint","text":"Calculate the probability of the logic formula given by LC for the PC\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.random_region_graph-Tuple{AbstractVector{UInt32}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.random_region_graph","text":"random_region_graph(X::AbstractVector{Var}, depth::Int = 5, replicas::Int = 2, num_splits::Int = 2)\n\nX: Vector of all variables to include; for the root region\ndepth: how many layers to do splits\nreplicas: number of replicas or paritions (replicas only used for the root region; for other regions only 1 parition (inner nodes), or 0 parition for leaves)\nnum_splits: number of splits for each parition; split variables into random equaly sized regions\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.region_graph_2_pc-Tuple{RegionGraph}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.region_graph_2_pc","text":"region_graph_2_pc(node::RegionGraph; num_nodes_root, num_nodes_region, num_nodes_leaf, balance_childs_parents)\n\nnum_nodes_root: number of sum nodes in the root region\nnum_nodes_leaf: number of sum nodes per leaf region\nnum_nodes_region: number of in each region except root and leaves\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample","text":"sample(pc::ProbCircuit, num_samples)\nsample(pc::ProbCircuit, num_samples, evidences)\n\nSample states from the probabilistic circuit distribution. Also can do conditional sampling if evidence is given (any subset of features).\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_comb-Tuple{Int64, Int64}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_comb","text":"Samples a single combination uniformly following Robert Floyd's algorithm.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_idem_primes!-Tuple{Vector{Int64}, Real, Vector{LogicCircuits.Bdd}, Integer}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_idem_primes!","text":"Samples primes for the ⊤ case.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_partition-Tuple{LogicCircuits.Bdd, BitSet, Real, Integer, Integer, Bool}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_partition","text":"Samples a partial partition.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_primes!-Tuple{LogicCircuits.Bdd, Vector{Int64}, Dict{LogicCircuits.Bdd, Vector{LogicCircuits.Bdd}}, Integer, Bool}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_primes!","text":"Samples primes for partial partition.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_psdd-Tuple{LogicCircuits.Bdd, LogicCircuits.Vtree, Integer, DataFrames.DataFrame}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_psdd","text":"Samples a PSDD from a BDD ϕ and vtree V with at most k elements in each disjunction node.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sample_row-Tuple{Int64}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sample_row","text":"Samples an element from a Binomial distribution with p=0.5.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.save_as_ensemble-Tuple{String, Ensemble{StructProbCircuit}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.save_as_ensemble","text":"Save file as a .esbl ensemble file format.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.sum_nodes-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.sum_nodes","text":"Get the list of summation nodes in a given circuit\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.summate","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.summate","text":"Sum nodes into a single circuit\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.to_sampled_dataframes-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.to_sampled_dataframes","text":"Convert an array of samples into a vector of dataframes\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.uniform_parameters!-Tuple{ProbCircuit}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.uniform_parameters!","text":"uniform_parameters!(pc::ProbCircuit; perturbation::Float64 = 0.0)\n\nAssign parameters of ProbCircuit so that we have a Uniform distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.update_pc_params_from_pbc!-Tuple{ProbCircuit, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.update_pc_params_from_pbc!","text":"function update_pc_params_from_pbc!(pc::ProbCircuit, bc, params; exp_update_factor = 0.0)\n\nDuring parameter learning pc's paramters are not updated automatically, and only the corresponding ParamBitCircuit's paramters update. This method updates the paramters of pc using the corresponding ParamBitCircuit.\n\nNote: This is mostly for internal use. \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.update_pc_params_from_pbc!-Tuple{ProbCircuit, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.update_pc_params_from_pbc!","text":"update_pc_params_from_pbc!(pc::ProbCircuit, pbc; exp_update_factor = 0.0)\n\nDuring parameter learning pc's paramters are not updated automatically, and only the corresponding ParamBitCircuit's paramters update. This method updates the paramters of pc using the corresponding ParamBitCircuit.\n\nNote: This is mostly for internal use. \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.update_pc_params_from_pbc!-Tuple{SharedProbCircuit, Any, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.update_pc_params_from_pbc!","text":"update_pc_params_from_pbc!(pc::SharedProbCircuit, bc, params, component_idx; exp_update_factor = 0.0)\n\nDuring parameter learning pc's paramters are not updated automatically, and only the corresponding BitCircuit's paramters update. This method updates the paramters of pc using the corresponding BitCircuit.\n\nNote: This is mostly for internal use. \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.vMI-Tuple{Any, Any, Any, Vector{UInt32}, Any, Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.vMI","text":"Pick the variable with maximum sum of mutual information\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.vRand-Tuple{Vector{UInt32}}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.vRand","text":"Pick the variable randomly\n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.wmc_chavira","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.wmc_chavira","text":"wmc_chavira(root::LogicCircuit; varprob::Function)::Float\n\nCompute weighted model count in the context of a chavira encoding (default negative literals to 1) Probability of each variable is given by varprob Function which defauls to 1/2 for every variable.\n\n\n\n\n\n","category":"function"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.zoo_jpc-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.zoo_jpc","text":"zoo_jpc(name)\n\nLoads JPC file with given name from model zoo. See https://github.com/UCLA-StarAI/Circuit-Model-Zoo.    \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.zoo_psdd-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.zoo_psdd","text":"zoo_psdd(name)\n\nLoads PSDD file with given name from model zoo. See https://github.com/UCLA-StarAI/Circuit-Model-Zoo.    \n\n\n\n\n\n","category":"method"},{"location":"api/internals/probabilistic_circuits/#ProbabilisticCircuits.zoo_spn-Tuple{Any}","page":"Probabilistic Circuits","title":"ProbabilisticCircuits.zoo_spn","text":"zoo_spn(name)\n\nLoads SPN file with given name from model zoo. See https://github.com/UCLA-StarAI/Circuit-Model-Zoo.    \n\n\n\n\n\n","category":"method"},{"location":"manual/queries/#man-queries","page":"Queries","title":"Queries","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"In this section, we go over most common probabilistic reasoning tasks, and provide code snippets to compute those queries. ","category":"page"},{"location":"manual/queries/#Setup","page":"Queries","title":"Setup","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"First, we load some pretrained PC, and the corresponding data.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"# This is needed to hide output from downloading artifacts\nusing LogicCircuits # hide\nusing ProbabilisticCircuits; #hide\npc = zoo_psdd(\"plants.psdd\")\ndata, _, _ = twenty_datasets(\"plants\");","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"using LogicCircuits # hide\nusing ProbabilisticCircuits; #hide\npc = zoo_psdd(\"plants.psdd\")\ndata, _, _ = twenty_datasets(\"plants\");\nprintln(\"circuit with $(num_nodes(pc)) nodes and $(num_parameters(pc)) parameters.\")\nprintln(\"dataset with $(num_features(data)) features and $(num_examples(data)) examples.\")","category":"page"},{"location":"manual/queries/#Full-Evidence-(EVI)","page":"Queries","title":"Full Evidence (EVI)","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"EVI refers to computing the probability when full evidence is given, i.e. when x is fully observed, the output is p(x). We can use EVI method to compute logp(x):","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"probs = EVI(pc, data);\nprobs[1:3]","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Computing the EVI of a mixture of circuits works the same way. You may either pass weights for the weighted mixture probability, or pass a component index to individually evaluate each component.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"mix, mix_weights, _ = learn_strudel(data; num_mix = 10, init_maxiter = 10, em_maxiter = 100)\n# This computes the weighted probability\nprobs = EVI(mix, data, mix_weights);\n# Alternatively, we may want to compute the probability of a single component\nc_prob = EVI(mix, data; component_idx = 1);\nc_prob[1:3]","category":"page"},{"location":"manual/queries/#Partial-Evidence-(MAR)","page":"Queries","title":"Partial Evidence (MAR)","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"In this case we have some missing values. Let x^o denote the observed features, and x^m the missing features. We would like to compute p(x^o) which is defined as p(x^o) = sum_x^m p(x^o x^m). Of course, computing this directly by summing over all possible ways to fill the missing values is not tractable. ","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"The good news is that given a smooth and decomposable PC, the marginal can be computed exactly and in linear time to the size of the PC.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"First, we randomly make some features go missing (you can also use make_missing_mcar from LogicCircuits library):","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"using DataFrames\nusing Tables\nfunction make_missing(d::DataFrame; keep_prob=0.8)\n    m = missings(Bool, num_examples(d), num_features(d))\n    flag = rand(num_examples(d), num_features(d)) .<= keep_prob\n    m[flag] .= Tables.matrix(d)[flag]\n    DataFrame(m, :auto)\nend;\ndata_miss = make_missing(data[1:1000,:]);\nnothing #hide","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Now, we can use MAR to compute the marginal queries:","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"probs = MAR(pc, data_miss);\nprobs[1:3]","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Note that MAR can also be used to compute probabilisties even if all data is observed, in fact it should give the same results as EVI. However, if we know all features are observed, we suggest using EVI as its faster in general.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"probs_mar = MAR(pc, data);\nprobs_evi = EVI(pc, data);\n\nprobs_mar ≈ probs_evi","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Just like EVI, MAR works the same way for mixtures.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"# Full weighted marginal probability\nprobs_mar = MAR(mix, data, mix_weights);\n# Individual component's marginal probability\nc_probs_mar = MAR(mix, data; component_idx = 1);\nc_probs_mar[1:3]","category":"page"},{"location":"manual/queries/#Conditionals-(CON)","page":"Queries","title":"Conditionals (CON)","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"In this case, given observed features x^o, we would like to compute p(Q mid x^o), where Q is a subset of features disjoint with x^o.  We can use Bayes rule to compute conditionals as two seperate MAR queries as follows:","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"p(q mid x^o) = cfracp(q x^o)p(x^o)","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Currently, this has to be done manually by the user. We plan to add a simple API for this case in the future.","category":"page"},{"location":"manual/queries/#Maximum-a-posteriori-(MAP,-MPE)","page":"Queries","title":"Maximum a posteriori (MAP, MPE)","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"In this case, given the observed features x^o the goal is to fill out the missing features in a way that p(x^m x^o) is maximized.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"We can use the MAP method to compute MAP, which outputs the states that maximize the probability and the log-likelihoods of each state.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"data_miss = make_missing(data,keep_prob=0.5);\nstates, probs = MAP(pc, data_miss);\nprobs[1:3]","category":"page"},{"location":"manual/queries/#Sampling","page":"Queries","title":"Sampling","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"We can also sample from the distrubtion p(x) defined by a Probabilistic Circuit. You can use sample to achieve this task.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"samples, _ = sample(pc, 100);\nsize(samples)","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Additionally, we can do conditional samples x sim p(x mid x^o), where x^o are the observed features (x^o subseteq x), and could be any arbitrary subset of features.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"#3 random evidences for the examples\nevidence = DataFrame(rand( (missing,true,false), (2, num_variables(pc))), :auto)\n\nsamples, _ = sample(pc, 3, evidence);\nsize(samples)","category":"page"},{"location":"manual/queries/#Expected-Prediction-(EXP)","page":"Queries","title":"Expected Prediction (EXP)","text":"","category":"section"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"Expected Prediction (EXP) is the task of taking expectation of a discrimintative model w.r.t a generative model conditioned on evidemce (subset of features observed).","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"mathbbE_x^m sim p(x^m mid x^o)  f(x^o x^m) ","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"In the case where f and p are circuit, and some structural constrains for the pair, we can do this expectation and higher moments tractably.  You can use Expectation and Moment to compute the expectations.","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"using DiscriminativeCircuits\nusing DataFrames\n\npc = zoo_psdd(\"insurance.psdd\")\nrc = zoo_dc(\"insurance.circuit\")\n\n# Using samples from circuit for the example; replace with real data\ndata, _ = sample(pc, 10);\ndata = make_missing(DataFrame(data, :auto));\n\nexps, exp_cache = Expectation(pc, rc, data)\n\nexps[1:3]","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"second_moments, moment_cache = Moment(pc, rc, data, 2);\nexps[1:3]","category":"page"},{"location":"manual/queries/","page":"Queries","title":"Queries","text":"stds = sqrt.( second_moments - exps.^2 );\nstds[1:3]","category":"page"},{"location":"manual/learning/#man-learning","page":"Learning","title":"Learning","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"In this section we provide few learning scenarios for circuits. In general, learning tasks for PCs can be separted into two categories: parameter learning and structure learning.","category":"page"},{"location":"manual/learning/#Learn-a-Circuit","page":"Learning","title":"Learn a Circuit","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"You can use learn_circuit to learn a probabilistic circuit from the data (both parameter and structure learning).","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using LogicCircuits\nusing ProbabilisticCircuits\ntrain_x, valid_x, test_x = twenty_datasets(\"nltcs\")\n\npc = learn_circuit(train_x; maxiter=100);\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. \" *  \n\"Train log-likelihood is $(log_likelihood_avg(pc, train_x))\"","category":"page"},{"location":"manual/learning/#Learning-a-circuit-from-missing-data","page":"Learning","title":"Learning a circuit from missing data","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"You can use learn_circuit_miss to learn a probabilistic circuit from missing data, i.e. some feature could be missing for each data point.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"train_x_miss = make_missing_mcar(train_x; keep_prob=0.9)\npc = learn_circuit_miss(train_x_miss; maxiter=100);\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. \" *  \n\"Train marginal-log-likelihood is $(marginal_log_likelihood_avg(pc, train_x))\"","category":"page"},{"location":"manual/learning/#Learn-a-mixture-of-circuits","page":"Learning","title":"Learn a mixture of circuits","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"We also support learning mixture of circuits using the Strudel algorithm (learn_strudel).","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using LogicCircuits\nusing ProbabilisticCircuits\nusing Statistics\n\ntrain_x, valid_x, test_x = twenty_datasets(\"nltcs\")\n\nspc, component_weights, lls = learn_strudel(train_x; num_mix = 10, init_maxiter = 20, em_maxiter = 100);\n\n\"SPC: $(num_nodes(spc)) nodes, $(num_parameters(spc)) parameters. \" *\n\"Train log-likelihood is $(mean(lls))\"","category":"page"},{"location":"manual/learning/#Learn-a-circuit-from-logical-constraints-and-data","page":"Learning","title":"Learn a circuit from logical constraints and data","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"There are several ways to learn a probabilistic circuit consistent with logical constraints. Juice currently supports the following:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Compilation from an SDD;\nCompilation from a BDD;\nRelaxation through sample_psdd.","category":"page"},{"location":"manual/learning/#Compilation-from-an-SDD","page":"Learning","title":"Compilation from an SDD","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"A circuit (more specifically a PSDD) can be easily constructed from an SDD by simply calling the compile function.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Let's assume we have the following CNF","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"phi(abcd)=(aveeneg b)wedge(cveeneg d)wedge(aveeneg d)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"as a .cnf file:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"/tmp/example.cnf\n---\nc Encodes the following CNF: ϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)\nc\np cnf 4 3\n1 -2 0\n3 -4 0\n1 -4 0","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"First we construct an SDD from the CNF. Here we sample a random vtree as an example (you might want to learn it from data instead with learn_vtree).","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"open(\"/tmp/example.cnf\", \"w\") do f \nwrite(f,\n\"\"\"\nc Encodes the following: ϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)\nc\np cnf 4 3\n1 -2 0\n3 -4 0\n1 -4 0\n\"\"\") \nend","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using LogicCircuits\nusing ProbabilisticCircuits\nusing DataFrames\n\nn = 4 # number of variables\nV = Vtree(n, :random)\nsdd = compile(SddMgr(V), read(\"/tmp/example.cnf\", LogicCircuit))\npc = compile(StructProbCircuit, sdd)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Let's check its support.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"# Matrix with all possible worlds.\nM = BitMatrix(undef, 2^n, n)\nfor i in 1:size(M, 1) M[i,:] .= [c == '0' ? false : true for c in reverse(bitstring(i-1)[end-n-1:end])] end\ndisplay(M)\n\n# Evaluate SDD.\ndisplay(sdd.(eachrow(M)))","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"And now the probabilities:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"# Evaluate the PSDD support.\nEVI(pc, DataFrame(M))","category":"page"},{"location":"manual/learning/#Compilation-from-a-BDD","page":"Learning","title":"Compilation from a BDD","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Compiing from a BDD is straightforward. Let's first create a BDD from the same previous constraints using LogicCircuits.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using LogicCircuits\n\nϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Now we either compile the PSDD directly from the BDD and give it random weights:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using ProbabilisticCircuits, DataFrames\n\n# Get all possible instances with BDD.all_valuations.\nM = all_valuations(collect(1:n))\nM_D = DataFrame(M)\n\npc = generate_from_bdd(ϕ, 4)\nEVI(pc, M_D)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Or compile from a BDD and learn weights from data:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"# Retrieve only possible worlds.\nW = M[findall(ϕ.(eachrow(M))),:]\n# Assign random probabilities for each world in W.\nR = rand(1:20, size(W, 1))\n# Construct a dataset that maps the distribution of R (world W[i] repeats R[i] times).\nD = DataFrame(vcat([repeat(W[i,:], 1, R[i])' for i ∈ 1:size(W, 1)]...))\n\npc = learn_bdd(ϕ, D; pseudocount = 0.0)\nEVI(pc, M_D)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Since BDDs are just right-linear vtree PSDDs, this \"compilation\" is merely a conversion from BDD syntax to PC syntax, attributing some weight to edges.","category":"page"},{"location":"manual/learning/#Sampling-a-circuit-from-a-relaxation-of-the-constraints","page":"Learning","title":"Sampling a circuit from a relaxation of the constraints","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"The two previous approaches are effective, but not always adequate. For instance, suppose our data consists of 6 variables: a, b, c, d, e and f, where only a, b, c and d are constrained (by phi), and the rest are free. Had we compiled phi from either an SDD or BDD, we'd end up with trivial structures for free variables. For instance, calling learn_bdd (or generate_from_bdd) with more variables than the size of the BDD's scope would result in a fully factorized distribution over the free variables.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"To address these issues, we might want to generate a circuit from both free and constrained variables with sample_psdd. Unfortunately, to keep the circuit tractable, sample_psdd only provides a relaxation of the constraints.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Let's first encode our constraints as a BDD just like our previous example and make up some random data.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using LogicCircuits, DataFrames\n\nn = 6\nϕ = (1 ∨ ¬2) ∧ (3 ∨ ¬4) ∧ (1 ∨ ¬4)\nM = all_valuations(collect(1:n))\nM_D = DataFrame(M)\nW = M[findall(ϕ.(eachrow(M))),:]\nR = rand(1:20, size(W, 1))\nD = DataFrame(vcat([repeat(W[i,:], 1, R[i])' for i ∈ 1:size(W, 1)]...))","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Now we can sample circuits from phi and data D.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"using ProbabilisticCircuits\nusing LogicCircuits: Vtree\n\npc = sample_psdd(ϕ, Vtree(n, :random), 16, D)\nEVI(pc, M_D)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"The third argument passed to sample_psdd indicates an upper bound on the number of children whose parents are sum nodes. The higher this bound, the more consistent with phi.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"In situations where background knowledge is not available, we may pass top to sample_psdd to only learn from data.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"pc = sample_psdd(⊤, Vtree(n, :random), 16, D)\nEVI(pc, M_D)","category":"page"},{"location":"manual/learning/#Learning-an-ensemble-of-circuits","page":"Learning","title":"Learning an ensemble of circuits","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"learn_strudel let's us learn an ensemble of circuits that share the same structure. For learning ensembles whose components have different structures, we have to use Ensemble.","category":"page"},{"location":"manual/learning/#Ensemble-of-sample_psdds","page":"Learning","title":"Ensemble of sample_psdds","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"We can learn an ensemble of random circuits through ensemble_sample_psdd.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"E = ensemble_sample_psdd(10, ϕ, 16, D; strategy = :em)\nEVI(E, M_D)","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Here we used EM to learn the weights of the ensemble. Alternatives are likelihood weighting (:likelihood), uniform weights (:uniform) or stacking (:stacking).","category":"page"},{"location":"manual/learning/#Misc-Options","page":"Learning","title":"Misc Options","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"In this sections, we provide options to have more control in learning circuits. For example, what if we only want to do parameter learning.","category":"page"},{"location":"manual/learning/#Parameter-Learning","page":"Learning","title":"Parameter Learning","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Given a fixed structure for the PC, the goal of parameter learning is to estimate the parameters so that likelihood is maximized.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"First, initliaze PC structure with a balanced vtree represneting a fully factorized distribution:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"v = Vtree(num_features(train_x), :balanced)\npc = fully_factorized_circuit(StructProbCircuit, v);\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.\" *  \n\"Train log-likelihood is $(log_likelihood_avg(pc, train_x))\"  ","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"No parmater learning is done yet, now let's, do maximum likelihood estimatation (MLE) using estimate_parameters:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"estimate_parameters!(pc, train_x; pseudocount=1.0);\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.\" *\n\"Train log-likelihood is $(log_likelihood_avg(pc, train_x))\"  ","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"As we see the likelihood improved, however we are still using a fully factorized distribution. There is room for improvement. For example, we can choose initial structure based on Chow-Liu Trees.","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"pc, vtree = learn_chow_liu_tree_circuit(train_x)\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters.\" *\n\"Train log-likelihood is $(log_likelihood_avg(pc, train_x))\"  ","category":"page"},{"location":"manual/learning/#Structure-Learning","page":"Learning","title":"Structure Learning","text":"","category":"section"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"There are several different approaches in structure learning. Currently we support the following approach:","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"Choose an initial structure and learn parameters\nPerform Greedy search for a bigger and better structure by doing operations such as split and clone.\nRepeat step 2 until satisfied or time limit reached","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"We start with the Chow-Liu structure we learned in last section, and run few structure learning iterations (20):","category":"page"},{"location":"manual/learning/","page":"Learning","title":"Learning","text":"pc, vtree = learn_chow_liu_tree_circuit(train_x)\nloss(circuit) = ProbabilisticCircuits.heuristic_loss(circuit, train_x)\npc = struct_learn(pc;  \n    primitives=[split_step],  \n    kwargs=Dict(split_step=>(loss=loss,)),\n    maxiter=20)\nestimate_parameters!(pc, train_x; pseudocount=1.0)\n\n\"PC: $(num_nodes(pc)) nodes, $(num_parameters(pc)) parameters. \" *\n\"Training set log-likelihood is $(log_likelihood_avg(pc, train_x))\"  ","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Julia 1.5 or greater. For installation, please refer to the official Julia Website.","category":"page"},{"location":"installation/#Installing-ProbabilisticCircuits","page":"Installation","title":"Installing ProbabilisticCircuits","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"You can use Julia's package manager, Pkg, to install this module and its dependencies. There are different options on how to do that, for example through command line or julia REPL. For more information and options on how to use Julia pacakge manager, please refer to Pkg's Documentation.","category":"page"},{"location":"installation/#From-Command-Line","page":"Installation","title":"From Command Line","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To install the latest stable release, run:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia -e 'using Pkg; Pkg.add(\"ProbabilisticCircuits\")'","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"You can also install the package with the latest commits on master branch. In that case, its also recommented to install the latest LogicCircuits:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia -e 'using Pkg; Pkg.add([PackageSpec(url=\"https://github.com/Juice-jl/LogicCircuits.jl.git\"),PackageSpec(url=\"https://github.com/Juice-jl/ProbabilisticCircuits.jl.git\")])'","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"note: Note\nTo get to Pkg mode, you need to run julia, then to press ]. Press backspace or ^C to get back to normal REPL mode.","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"While in Pkg mode, run the following to install the latest release:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"add ProbabilisticCircuits","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Similarly, to install from the latest commits on master branch, run:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"add LogicCircuits#master\nadd ProbabilisticCircuits#master","category":"page"},{"location":"installation/#Optional-Dependencies","page":"Installation","title":"Optional Dependencies","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"We use Requires.jl to make some of our rarely used dependencies optional, so they do not automatically get installed.  Currently, BlossomV.jl is the only such dependency due to its rare ussage and issues with Windows. This means if you get an error related to BlossomV.jl, you can fix that by adding BlossomV to your project using Pkg.add.","category":"page"},{"location":"installation/#Testing","page":"Installation","title":"Testing","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"If you are installing the latest commit, we recommend running the test suite to make sure everything is in order, to do that run:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia --color=yes -e 'using Pkg; Pkg.test(\"ProbabilisticCircuits\")'","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Note: If you want the tests to run faster, you can use multiple cores. To do that set the following environment variable (default = 1 core):","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"export JIVE_PROCS=8","category":"page"},{"location":"manual/demo/#man-demo","page":"Quick Demo","title":"Quick Demo","text":"","category":"section"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"In this section, we provide quick code snippets to get started with ProbabilisticCircuits and provide basic understanding of them. PCs are represented as a computational graphs that define a joint probability distribution as recursive mixtures (sum units) and factorizations (product units) of simpler distributions (input units).","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"Generally, we learn structure and parameters of circuit from data. Alternatively, we can also specify circuits in code. For example, the following snippet defines a circuit depending on 3 random variables. The literals function returns the input units of the circuit, in this case we get 6 different units (3 for positive literals, and 3 for negative literlas).  You can use * and + operators to build a circuits.","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"using LogicCircuits;\nusing ProbabilisticCircuits;\n\nX1, X2, X3 = literals(ProbCircuit, 3)\npc = 0.3 * (X1[1] *\n             (0.2 * X2[1] + 0.8 * X3[2])) +\n     0.7 * (X1[2] *\n             (0.4 * X2[2] + 0.6 * X3[1]));\n\nnothing # hide","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"We can also plot circuits using plot(pc) to see the computation graph (structure and parameters). The output of plot(pc) has a type of TikzPictures.TikzPicture. Generally, notebooks automatically renders it and you see the figure in the notebook. ","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"using TikzPictures  # workaround\nTikzPictures.standaloneWorkaround(true)  # workaround\nplot(pc);","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"However, if you are not using a notebook or want to save to file you can use the following commands to save the plot in various formats.","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"using TikzPictures;\nz = plot(pc);\nsave(PDF(\"plot\"), z);\nsave(SVG(\"plot\"), z);\nsave(TEX(\"plot\"), z);\nsave(TIKZ(\"plot\"), z);","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"You can ask basic questions about PCs, such as (1) how many variables they depends on, (2) how many nodes, (3) how many edges, (4) or how many parameters they have.","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"num_variables(pc)","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"num_nodes(pc)","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"num_edges(pc)","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"num_parameters(pc)","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"In the case that we have literals as input units, PCs can also be thought of as adding parameters to a LogicCircuit to define a distribution (See LogicCircuit.jl docs for more details). To get the corresponding logical formula, we can:","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"tree_formula_string(pc)","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"To enable tractable queries and opertations, PCs need to have certain structural properties. For example, we can check for smoothness and determinism as follows:","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"c1 = 0.4 * X1[1] + 0.6 * X1[2];\n\"Is $(tree_formula_string(c1)) smooth? $(issmooth(c1))\"\n","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"c2 = 0.4 * X1[1] + 0.6 * X2[2];\n\"Is $(tree_formula_string(c2)) smooth? $(issmooth(c2))\"","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"c1 = X1[1] * X2[1] + X1[1] * X2[2];\n\"Is $(tree_formula_string(c1)) deterministic? $(isdeterministic(c1))\" ","category":"page"},{"location":"manual/demo/","page":"Quick Demo","title":"Quick Demo","text":"c2 = X1[1] * X2[1] + X1[1] * X2[1]\n\"Is $(tree_formula_string(c2)) deterministic? $(isdeterministic(c2))\"","category":"page"},{"location":"#ProbabilisticCircuits.jl","page":"Home","title":"ProbabilisticCircuits.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This module provides a Julia implementation of Probabilistic Circuits (PCs),  tools to learn structure and parameters of PCs from data, and tools to do tractable exact inference with them. ","category":"page"},{"location":"#What-are-Probabilistic-Circuits?","page":"Home","title":"What are Probabilistic Circuits?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Probabilistic Circuits provides a unifying framework for several family of tractable probabilistic models. PCs are represented as a computational graphs that define a joint probability distribution as recursive mixtures (sum units) and factorizations (product units) of simpler distributions (input units).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given certain structural properties, PCs enable different range of tractable exact probabilistic queries such as computing marginals, conditionals, maximum a posteriori (MAP), and more advanced probabilistic queries.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In additon to parameters, the structure of PCs can also be learned from data. There are several approaches in learning PCs, while keeping the needed structural constrains intact. Currently, This module includes implementation for few of these approaches with plans to add more over time.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, parallelism (on both CPU and GPU) is leveraged to provide faster implementation of learning and inference.","category":"page"},{"location":"#Where-to-learn-more-about-them?","page":"Home","title":"Where to learn more about them?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For an overview of the motivation and theory behind PCs, you can start by watching the ECML-PKDD tutorial on Probabilistic Circuits. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Probabilistic Circuits: Representations, Inference, Learning and Theory (Video)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details and additional references, you can refer to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models (PDF)","category":"page"},{"location":"api/public/#api-public","page":"Public APIs","title":"Public APIs","text":"","category":"section"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"This page lists documentation for the most commonly used public APIs of ProbabilisticCircuits.jl. Visit the internals section for a auto generated documentation for more public API and internal APIs.","category":"page"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"Pages = [\"public.md\"]","category":"page"},{"location":"api/public/#Circuit-IO","page":"Public APIs","title":"Circuit IO","text":"","category":"section"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"read\nwrite","category":"page"},{"location":"api/public/#Base.read","page":"Public APIs","title":"Base.read","text":"Base.read(file::AbstractString, ::Type{C}) where C <: ProbCircuit\n\nReads circuit from file; uses extension to detect format type, for example \".psdd\" for PSDDs.\n\n\n\n\n\n","category":"function"},{"location":"api/public/#Base.write","page":"Public APIs","title":"Base.write","text":"Base.write(file::AbstractString, circuit::ProbCircuit)\n\nWrites circuit to file; uses file name extention to detect file format.\n\n\n\n\n\nBase.write(files::Tuple{AbstractString,AbstractString}, circuit::StructProbCircuit)\n\nSaves circuit and vtree to file.\n\n\n\n\n\n","category":"function"},{"location":"api/public/#Learning-Circuits","page":"Public APIs","title":"Learning Circuits","text":"","category":"section"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"learn_circuit\nlearn_strudel\nestimate_parameters!\nestimate_parameters_em!\nestimate_parameters_em_multi_epochs!\nlearn_chow_liu_tree_circuit","category":"page"},{"location":"api/public/#ProbabilisticCircuits.learn_circuit","page":"Public APIs","title":"ProbabilisticCircuits.learn_circuit","text":"Learn structure of a single structured decomposable circuit\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.learn_strudel","page":"Public APIs","title":"ProbabilisticCircuits.learn_strudel","text":"learn_strudel(train_x; num_mix = 5, init_maxiter = 10, em_maxiter=20)\n\nLearn a mixture of circuits See \"Strudel: Learning Structured-Decomposable Probabilistic Circuits. arxiv.org/abs/2007.09331\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.estimate_parameters!","page":"Public APIs","title":"ProbabilisticCircuits.estimate_parameters!","text":"estimate_parameters!(pc::ProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0)\n\nMaximum likelihood estimation of a ProbCircuit's parameters given data\n\n\n\n\n\nestimate_parameters!(spc::SharedProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0)\n\nMaximum likelihood estimation of a SharedProbCircuit's parameters given data.\n\nbagging support: If spc is a SharedProbCircuit and data is an array of DataFrames   with the same number of \"components\", learn each circuit with its corresponding    dataset.\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.estimate_parameters_em!","page":"Public APIs","title":"ProbabilisticCircuits.estimate_parameters_em!","text":"estimate_parameters_em!(pc::ProbCircuit, data; pseudocount::Float64, entropy_reg::Float64 = 0.0, exp_update_factor::Float64 = 0.0, update_per_batch::Bool = false)\n\nOne epoch of Expectation maximization (EM) parameter learning for circuits. Useful when having missing data or non-deterministic circuits.\n\nentropy_reg: Entropy Regularization\nupdate_per_batch: Whether to update circuit paramters per batch (using the ParamBitCircuit's paramters)\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.estimate_parameters_em_multi_epochs!","page":"Public APIs","title":"ProbabilisticCircuits.estimate_parameters_em_multi_epochs!","text":"estimate_parameters_em_multi_epochs!(circuit::ProbCircuit, train_data)\n\nRuns multiple epochs of EM for circuit. It will run on CPU/GPU based on where train_data is.\n\nArguments: \n\ncircuit:\ntrain_data: training data. If want gpu, move to gpu before calling this to_gpu(train_data).\n\nKeyword arguments:\n\nvalid_data=nothing: validation data, should be same device as train_data \ntest_data=nothing: test data, data, should be same device as train_data \nentropy_reg::Float64 = 0.0: Entropy regularization\nexp_update_factor_start::Float64 = 0.1: \nexp_update_factor_end::Float64 = 0.9:\nem_warmup_iters=100 : number of EM Warm up iterations\nem_finetune_iters=100: number of EM fine tune iterations\nverbose=false: verbose or not\nverbose_log_rate=1: how often to print info\nsave_path=nothing: path of the file to save the partially trained circuit\nsave_rate=20: how often to save the circuit to file\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.learn_chow_liu_tree_circuit","page":"Public APIs","title":"ProbabilisticCircuits.learn_chow_liu_tree_circuit","text":"Learning from data a structured-decomposable circuit with several structure learning algorithms\n\n\n\n\n\n","category":"function"},{"location":"api/public/#Circuit-Queries","page":"Public APIs","title":"Circuit Queries","text":"","category":"section"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"marginal\nmax_a_posteriori\nsample","category":"page"},{"location":"api/public/#ProbabilisticCircuits.marginal","page":"Public APIs","title":"ProbabilisticCircuits.marginal","text":"marginal(root::ProbCircuit, data::Union{Real,Missing}...)\nmarginal(root::ProbCircuit, data::Union{Vector{Union{Bool,Missing}},CuVector{UInt8}})\nmarginal(circuit::ProbCircuit, data::DataFrame)\nmarginal(circuit::ParamBitCircuit, data::DataFrame)::AbstractVector\nmarginal(circuit::SharedProbCircuit, data::DataFrame, weights::Union{AbstractArray, Nothing}; component_idx)\n\nEvaluate marginals of the circuit bottom-up for given input(s).\n\nMissing values should be denoted by missing in the data.\n\nOutputs logp(x^o) for each data point. \n\n\n\n\n\nmarginal(circuit::SharedProbCircuit, data::DataFrame, weights::Union{AbstractArray, Nothing} = nothing; component_idx = 0)::AbstractVector\n\nComputes marginals for one circuit in a SharedProbCircuit.\n\ncomponent_idx: index of the SharedProbCircuit component to comptue marginals on.\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.max_a_posteriori","page":"Public APIs","title":"ProbabilisticCircuits.max_a_posteriori","text":"max_a_posteriori(root::ProbCircuit, data::Union{Bool,Missing}...)\nmax_a_posteriori(root::ProbCircuit, data::Union{Vector{<:Union{Bool,Missing}},CuVector{UInt8}})\nmax_a_posteriori(circuit::ProbCircuit, data::DataFrame)\nmax_a_posteriori(pbc::ParamBitCircuit, data; Float=Float32)\n\nEvaluate maximum a-posteriori state of the circuit for given input(s).\n\nOutputs the states, and the corresponding probabilities (in log domain).\n\n\n\n\n\n","category":"function"},{"location":"api/public/#ProbabilisticCircuits.sample","page":"Public APIs","title":"ProbabilisticCircuits.sample","text":"sample(pc::ProbCircuit, num_samples)\nsample(pc::ProbCircuit, num_samples, evidences)\n\nSample states from the probabilistic circuit distribution. Also can do conditional sampling if evidence is given (any subset of features).\n\n\n\n\n\n","category":"function"},{"location":"api/public/#Compilation","page":"Public APIs","title":"Compilation","text":"","category":"section"},{"location":"api/public/","page":"Public APIs","title":"Public APIs","text":"compile_sdd_from_clt","category":"page"},{"location":"api/public/#ProbabilisticCircuits.compile_sdd_from_clt","page":"Public APIs","title":"ProbabilisticCircuits.compile_sdd_from_clt","text":"Compile a psdd circuit from clt and vtree\n\n\n\n\n\n","category":"function"}]
}
